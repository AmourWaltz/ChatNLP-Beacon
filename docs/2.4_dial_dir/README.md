# 对话系统研究挑战 Research Challenges for Dialogue System

## 对话模型中的事实错误

&emsp;&emsp;随着 ChatGPT 的横空出世，智能对话大模型俨然已成为 AI 发展的焦点，更是在整 个自然语言处理 (NLP) 领域掀起了一阵海啸。自去年席卷全球以来便引起各行各业空前 的热度，数亿用户纷纷惊叹于 ChatGPT 的强大功能，思考其背后关键技术革新，也关注 当前 ChatGPT 仍存在哪些缺陷，除了巨量数据资源的耗费需求，无法与时俱进关联最新 信息等之外，一个引人瞩目的问题就是 ChatGPT 交互中仍会生成不少的事实性错误，对 一些老幼皆知的简单问题也会一本正经的胡说八道，如下图所示

<div align=center>
<img src="./../images/nlp_ds_chatgpt1.png" width="60%"/>
</div>

&emsp;&emsp;这种事实性错误的存在对产品的广泛落地不可谓不致命，假设应用于医学，金融等非闲 聊式场景，这些潜在风险就可能会造成经济损失或威胁生命安全，因此消除大模型中的 事实错误成为了工业界和学术界的共同需求。

&emsp;&emsp;首先明确一则术语，在 NLP 论文中，这种事实性错误一般统称为“幻觉”(Hallucination)，顾名思义，该术语最早用于图像合成等领域，直到后来描述诸如图像检测时检 测到虚假或错误目标等现象，才沿用至自然语言生成 (NLG) 任务，指模型生成自然流畅，语法正确但实际上毫无意义且包含虚假信息即事实错误的文本，以假乱真，就像人产生的幻觉一样。

&emsp;&emsp;本文主要参考港科大的综述文章，全部采用模型“幻觉”来指代前文所述的事实性错误，先简述下 NLG 生成“幻觉”文本的成因，接着详细介绍对话任务中的“幻觉” 现象，针对对话任务的“幻觉”评估和解决方法等。

#### 自然语言生成的“幻觉”成因

&emsp;&emsp;此前已有相关工作将自然文本生成中的幻觉问题分为内部幻觉 (intrinsic hallucination) 和外部幻觉 (extrinsic hallucination) 两大类。所谓内部幻觉，是指生成的文本与给 定源文本出现不忠实 (unfaithfulness) 或不一致 (inconsistency) 的现象，这种问题常见于文本摘要任务中，生成的摘要与原文不一致。另一种外部幻觉，则是指生成内容在源文本 中并未提及，我们虽然不能找出相关证据，但也不能断言这就是错误的，这就涉及到语言模型本身的事实性知识 (factual knowledge) 或世界知识 (real-world knowledge)。下面对 这些幻觉的成因进行分析。

&emsp;&emsp;虽然两类幻觉表现上有着明显区别，但成因却较为类似，根据以下因果图即可简单分析

<div align=center>
<img src="./../images/nlp_ds_causal.png" width="60%"/>
</div>

&emsp;&emsp;生成的文本 $Y$ 由源文本 $X$ 和语言模型里的先验知识 $K$ 共同决定，由于一般认为给定的源文本都是事实正确的 ground-truth，所以出现的幻觉一般都会归结于语言模型本身包含了错误事实。语言模型中的先验知识都来自于训练语料，用于训练语言模型的大数据语料库在收集时难免会包含一些错误的信息，这些错误知识都会被学习，存储在模型参数中，相关研究表明模型生成文本时会优先考虑自身参数化的知识，所以更倾向生成幻觉内容，这也是自然语言生成任务中大部分幻觉的来源，本文后续讨论的幻觉默认基于这个原因。

&emsp;&emsp;另一方面，模型训练和推理时的差异，也是导致推理时更容易生成幻觉的原因之一。训练中通常用最大似然估计的做法训练解码器，并且是以 ground-truth 作为后续预测 token 的前缀输入，而解码时则根据历史序列生成来预测下一个 token。通过改变解码策略，也能一定程度上增强生成文本的事实性。

#### 对话系统中的“幻觉”现象

&emsp;&emsp;随着对话大模型 ChatGPT 的巨大成功，对话系统的“幻觉”现象也成为热门的研究方向。
构建对话系统需要根据用户话语和对话历史生成流畅连贯，与对话历史保持一致，且满足用户对话需求的合理回复。
对话任务分为任务型对话 (Task-Orientated Dialogue) 和开放域对话 (Open-Domain Dialogue)，任务型对话在很多场景的自动客服上已广泛应用，评测也相对容易，研究难点主要集中在类似 ChatGPT 这种开放域对话模型上，虽然是用于闲聊场景，但是需要满足各种用户能够提出的问题，解决各类任务，由于针对一条用户话语的回复并不唯一，也很难制定一个完善的评价体系。
因此开放域对话中的幻觉研究也更具有挑战性，在对话模型的研究中，用来描述这些文本生成的幻觉问题则有一个更常见的术语，“不一致” (inconsistency)。

&emsp;&emsp;对话回复的不一致也可分为两类，模型自身不一致 (self-inconsistency) 和外部不一致 (external inconsistency)。
自身不一致是指模型生成的回复与对话历史或与自身已生成回复相矛盾，属于前文所提及的内部幻觉；与对话历史的不一致性问题一般来自于历史信息的遗忘，包含与已生成文本相矛盾，这是人设 (persona) 对话中常见的问题，赋予系统一个固定角色，在聊天过程中模型的人设信息会发生变化，如下图所示

<div align=center>
<img src="images/nlp_ds_persona.png" width="40%"/>
</div>

&emsp;&emsp;而另一种外部不一致的问题，则是对话系统为了生成角色一致且信息丰富的回复，会将包含显式角色信息的外部数据引入系统以辅助模型生成，例如 PersonaChat 数据集中每个对话都伴随着人物描述，通过以角色描述为条件来增强角色一致性。
尽管 PersonaChat 已经在不少方法上取得显著效果，但进一步研究不依赖于给定角色描述的方法依然是有必要的，因为单靠文本描述很难覆盖一个角色的所有特征。

&emsp;&emsp;开放域知识对话要求模型在外部知识图或知识语料库的帮助下，将外部知识视为源文本的一部分，生成与知识描述一致并且完成对话任务的回复。
因此知识对话中的“幻觉”或“不一致”则聚焦于回复与外部知识的关系，比如一种常见的问题就是生成回复包含与所提供的外部知识不一致（图4）。

<div align=center>
<img src="images/nlp_ds_incon.png" width="40%"/>
</div>

&emsp;&emsp;由于世界知识数量庞大且不断更新，知识对话的很难保持事实正确性 (factual correctness)，也就是前文所说的外在幻觉，它可能是真实的但难以验证。
知识对话涉及两个步骤，首先需要用一个检索模型根据上下文对话来检索所需要的相关知识，然后将检索的知识作为输入文本生成回复，对这两个阶段进行优化都可以显著提高对话的事实正确性。
文章构造了一个专门用于知识对话事实一致性检测的数据集 Conv-FEVER，通过改编 Wizard-of-Wikipedia 创建，由事实一致和不一致的回复组成，该工作将回复与外部知识的关系分为三类：第一种是非验证性回复，即生成的回复只是做了简短闲聊，并未涉及任何外部知识，也就不需要来验证与知识的一致性；
第二种是事实一致性回复，即回复与外部知识高度相关一致，利用外部知识完成对话任务，这是最理想的状态；
第三种则是外部幻觉性回复，即回复中的知识与提供的外部知识信息不一致，由于外部知识也是检索而来，我们无法断言回复就一定发生了事实错误，这就需要借助“世界知识”来判断；这三种分类基本涵盖了大部分知识对话可能产生的幻觉现象，利用该数据集，可训练分类器以检测与所提供的知识相关的事实不一致的回复。


#### 对话系统“幻觉”的评估方法

&emsp;&emsp;对话系统，尤其是开放域对话，模型幻觉的评估仍是一个悬而未决的难题，截至目前也没有标准指标。
对话回复往往根据事实一致性或事实正确性进行评估，整体有三条思路，分别是基于统计，模型和人工的评估方法。


&emsp;&emsp;基于统计方法的评估：最简单的幻觉评估方法是直接计算生成文本和参考目标文本之间的词汇匹配度，例如 ROUGE，BLEU 等，F1 分数结合了生成文本和目标文本的精确率和召回率，能更好的反应其匹配关系。额外使用源文本是因为不能保证输出目标文本包含输入源文本中可用的完整信息集。 
然而开放域对话任务的回复往往不唯一，输入与输出是一对多的映射，而实际应用中，覆盖所有可能的输出几乎不可能，这也给目标参考的选择造成很大困难。
在知识对话中，假设给定的外部知识是事实正确的，那么我们则更关注生成回复与依赖的外部知识间的一致性，相关工作提出 Knowledge F1 指标用于测量生成回复数据集收集期间参考的真实知识之间的重叠，非常适用于知识对话场景。

&emsp;&emsp;基于模型方法的评估：模型评估方法主要基于自然语言推理 (Natural Language Inference, NLI)，即判断一项假设（即生成文本）是否蕴含于前提（即参考文本），基于 NLI 的指标将幻觉分数定义为源文本与生成文本之间的蕴含概率，也称为生成文本蕴含、中立和矛盾的次数百分比；当然，这也意味着需要先收集相关蕴含关系的数据集来训练这样一个判别模型，当前已有不少工作工作发布额外的一致性数据集在基于 NLI 的方法上训练可学习的评估指标，如前文提到的 Conv-FEVER 数据集；在知识对话中，还有工作提出借助问答系统的 $Q^2$ 方法来判断回复与外部知识的一致性。

&emsp;&emsp;基于模型的评估方法整体上比词级别的统计方法要适用得多，然而文本蕴含模型只能返回一个分数，无法定位具体生成文本的哪些部分是错误的，从消除事实错误的角度讲，还是应该更关注细粒度，可以定位到词级别的评估方法。

&emsp;&emsp;基于人工的评估：由于当前模型幻觉问题自动评估方法仍不完善，无论是通用的文本生成任务还是对话系统，人工评估仍然是最常用的方法之一，通常有对幻觉文本进行评分或与 ground-truth 直接比较两种方法，但都不可避免的会增加研究成本，一种比较折中的方法是，先让人工评估者在某个范围内对回复中的幻觉现象进行评分，然后提出改进一些现有的自动评估方法，只要该自动评估指标与人类评估打分趋势保持一致，就可以认为是个可行的评估方法。


#### 对话模型“幻觉”的解决办法

&emsp;&emsp;前文已分析过，造成模型生成幻觉文本的首要因素是训练数据，那么构造干净的数据集，进行数据去噪显然是一条可行的方法。
由于预训练数据多为网上收集的句子，一般都需要进行去噪操作，修改语法，指代不明或事实错误，确保语言模型能学习到事实准确的知识，另外也可以用 Wikipedia 这样的知识库或其他三元组表示的知识来对语言模型的进行知识增强，这些数据都是公认的包含世界知识的准确数据，对于降低模型幻觉的干扰有很大帮助；
在对话微调阶段，可以先用前文的测量指标对回复进行评估打分，然后过滤掉那些低质量不可信的回复，人工标注的样本也需要严格的筛选，确保模型生成与外部知识一致，并且保留对话特性，能完成应用场景对话任务的可靠回复。

&emsp;&emsp;数据方法涉及到人工构造，也就意味着成本较高，所以学术上更加关注使用其他方法，这里统称为模型方法，是因为区别于数据方法，其他方法都是对模型的架构、训练或推断来进行优化的。
对开放域对话任务而言，我们往往希望能控制生成回复的偏好，使其具备某种特性，以手动提供的可控 token 或自动预测的方式加到输入上，来提高生成回复的可信度，例如训练一个 token 来控制生成回复偏向第一人称的主观表达还是第三人称的客观陈述，然后在测试阶段通过手动赋值就可以控制回复的偏好了，这对于人设对话是很有参考价值的，如果将人设特征如性格、年龄、性别、职业、爱好等都设置成可控 token，那么只需要手动设置，就可以构建出各式各样的虚拟角色了。

&emsp;&emsp;另外在知识对话中，模型幻觉很大程度来源之一是外部知识选择不正确，因此用更强的检索模型搜索知识，返回更加有用的知识，也是消除对话回复幻觉的有效途径之一。
当前 ChatGPT 并不具备检索能力，其模型内部的隐式知识已然非常强大，一旦可以进行检索，结合网络上海量数据，就可以做到实时学习，并更新模型内部过期的知识，这对模型效果的进一步提升也是相当可观的。

最后再来调戏一把 ChatGPT。

<div align=center>
<img src="images/nlp_ds_chatgpt.png" width="60%"/>
</div>

<div align=center>
<img src="images/nlp_ds_chatgpt2.png" width="60%"/>
</div>

&emsp;&emsp;本文受启发于 ChatGPT 的事实错误问题，从文本生成的幻觉问题入手，对当前对话大模型的幻觉现象，评估以及解决方法进行了简单综述，模型幻觉仍是对话大模型的发展瓶颈，本文也不难看出，无论是幻觉评估还是解决，受限于对话任务本身的难度，都还是任重而道远，不过我们有理由相信，一旦解决了模型幻觉这一重大难题，那通用大模型也就离真正的人工智能不过咫尺之遥了。
