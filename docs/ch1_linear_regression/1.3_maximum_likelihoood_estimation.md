# [最大似然估计 Maximum Likelihoood Estimation](./ch1_linear_regression/1.3_maximum_likelihoood_estimation.md)

> [1.1 多项式拟合 Polynomial Fitting](./ch1_linear_regression/1.1_polynomial_fitting.md) </br>
> [1.2 线性基函数模型 Linear Basis Function Model](./ch1_linear_regression/1.2_linear_basis_function_model.md) </br>
> [1.3 最大似然估计 Maximum Likelihoood Estimation](./ch1_linear_regression/1.3_maximum_likelihoood_estimation.md) </br>
> [1.4 最小均方差 Minimum Square Error](./ch1_linear_regression/1.4_minimum_square_error.md) </br>
> [1.5 梯度下降法 Gradient Desent](./ch1_linear_regression/1.5_gradient_desent.md) </br>
> [1.6 解析法 Analytic Method](./ch1_linear_regression/1.6_analytic_method.md) </br>

线性回归问题都可以通过最⼩化误差函数来解决，通常选择平方和作为误差函数，接下来就从概率论的角度解释为什么选择求最小平方 (least squares) 。我们仍以曲线拟合为例，已知含有 $N$ 个输入数据点 $\boldsymbol{x}=\left \{ x_1, x_2, ..., x_N \right \}$ 及对应的目标值 $\boldsymbol{t}=\left \{ t_1, t_2, ..., t_N \right \}$  组成的数据集，对新输入的变量 $x_j$ 的目标值 $t_j$ 做出预测。正如前文所讲，我们对该例中从 $\boldsymbol{x}$ 到 $\boldsymbol{t}$ 的真实映射关系并不知晓，只是人为的选取一些基函数构造模型来逼近真实映射，可以说是盲选的，所以大部分情况下对于新输入变量目标值 $t_j$ 的预测总会存在偏差，也就意味着 $t_j$ 的预测需要引入一定的不确定性 (uncertainty)，于是我们使用关于目标预测的概率分布来表示这种不确定性，从而使预测结果更加可信。
我们通常会选择高斯分布 (Gaussian distribution) （或正态分布 (normal distribution)）来建模，原因有二，其一，我们想要建模的很多分布的真实情况是比较接近高斯分布的。中心极限定理（central limit theorem）说明很多独立随机变量的和近似服从高斯分布，意味着在实际中，很多复杂系统都可以被成功地建模成高斯分布的噪声；其二，在具有相同方差的所有可能的概率分布中，高斯分布在实数上具有最大的不确定性。我们假定给定 $x_j$ 的值， 对应的 $t_j$ 值服从⾼斯分布，分布的均值为 $y(x_j,\boldsymbol{w})$ ，此时有

$$p(t_j|x_j, \boldsymbol{w}, \beta) = \mathcal{N} (t_j|y(x_j, \boldsymbol{w}), \beta^{-1})\tag 1 $$
其中 $\beta$ 对应高斯分布方差的导数，很显然分布均值 $y(x_j,\boldsymbol{w})$ 就是我们选取的拟合函数。更一般地解释，为了增加结果的不确定性，我们对 $x$ 的预测结果 $t$ 加了一个均值为0，方差为 $\beta$ 的高斯噪声 $\epsilon$ ，使得 $t=y(x,\boldsymbol{w})+\epsilon$ ，最终预测结果符合以上假设的概率分布。此处不同目标值的误差项 $\epsilon$ 是相互独立的。对于 $\boldsymbol x$ 的⼀个新值，最优的预测由⽬标变量的条件均值给出，在目标值高斯分布的情况下，条件均值可以写为

$$\mathbb{E} \left [ t|x  \right ] =\int tp(t|x)dt=y(x,\boldsymbol{w})\tag 2$$
为了证明假设的概率分布适用于给定数据集上的所有数据，我们需要用训练数据 $\left \{\boldsymbol{x},\boldsymbol{t} \right \}$ ，通过最⼤似然 (maximum likelihood) ⽅法，来决定参数 $\boldsymbol{w}$ 和 $\beta$ 的值。所谓最大似然估计，就是寻找能够以较高概率产生观测数据的概率模型（或似然函数），通过最⼤化数据集的真实概率分布计算固定的参数，即认为参数 $\boldsymbol{w}$ 和 $\beta$ 是定值而数据集是变量，与之相对应的最大后验估计 (maximum posterior) 则是在给定数据集的情况下最⼤化参数的概率分布，认为参数 $\boldsymbol{w}$ 和 $\beta$ 是变量而数据集是定值，这个会在后续篇幅展开讨论。此时似然函数为

$$p(\boldsymbol{t}|\boldsymbol{x}, \boldsymbol{w}, \beta) = \prod_{n=1}^{N}\mathcal{N} (t_n|y(x_n, \boldsymbol{w}), \beta^{-1})\tag 3$$
为了最大化似然函数，我们会将一元变量 $x$ 的高斯分布的一般形式 

$$\mathcal{N}(x|\mu ,\sigma ^2) =\frac{1}{\sqrt{2\pi }\sigma } \exp \left \{ -\frac{1}{2\sigma ^2}(x-u)^2  \right \}\tag 4$$
取对数然后替换之，得到对数似然函数

$$\ln_{}{p(\boldsymbol{t} |\boldsymbol{x},\boldsymbol{w},\beta) } =-\frac{\beta}{2} \sum_{n=1}^{N} \left \{ y(x_n,\boldsymbol{w})-t_n \right \}^2+\frac{N}{2}\ln_{}{\beta }-\frac{N}{2}\ln_{}{2\pi }\tag 5$$
⾸先考虑确定 $\boldsymbol{w}$ 的最⼤似然解（记作 $\boldsymbol{w}_{ML}$ ），先省略与 $\mathbf{w}$ 无关的公式后两项，同时注意到，使⽤⼀个正的常数系数来缩放对数似然函数并不会改变 $\boldsymbol w_{ML}$ 的位置， 因此我们可以⽤ $\frac {1}{2}$ 来代替系数 $\frac {\beta}{2}$ 。最后，我们不去最⼤化似然函数，⽽是等价地最⼩化负对数似然函数。于是我们看到，对于确定 $\boldsymbol w_{ML}$ 的问题来说，最⼤化似然函数等价于最⼩化第一节中的平⽅和误差函数。因此，在⾼斯噪声的假设下，平⽅和误差函数是最⼤化似然函数的⼀个⾃然结果。在确定了控制均值的参数向量 $\boldsymbol w_{ML}$ 之后，就可以求解 $\beta$ ，因为平方和误差函数的最小值为0，很容易确定最大化似然函数的 $\beta _{ML}$ ，其形式为

$$\frac{1}{\beta_{ML}} =\frac{1}{N} \sum_{n=1}^{N} \left \{ y(x_n,\boldsymbol{w}_{ML})-t_n \right \}^2\tag 6$$
此时关于新输入的 $x_j$ ，我们代入参数确定的最大似然函数，得到关于 $t_j$ 的预测分布

$$p(t|x_j,\boldsymbol w _{ML}, \beta _{ML})=\mathcal{N}(t_j|y(x_j, \boldsymbol w _{ML}),\beta ^{-1}_{ML})\tag 7$$
这时再考虑将参数 $\boldsymbol w$ 看做服从某种概率分布的变量，同样地，为了最大化 $\boldsymbol w$ 的不确定性，我们假设其服从以下高斯分布

$$p(\boldsymbol w|\alpha)=\mathcal{N}(\boldsymbol w|\boldsymbol 0, \alpha^{-1}\boldsymbol{I})=(\frac {\alpha}{2\pi})^{\frac {M}{2}}\exp \left \{ -\frac{\alpha}{2} \boldsymbol {w} ^T \boldsymbol{w} \right \}\tag 8$$
其中 $\alpha$ 是分布方差， $M$ 是参数数量，选用0均值是因为我们通常在实际操作中将 $\boldsymbol w$ 初始化为 $\boldsymbol 0$ 。如果再将 $\boldsymbol w$ 看做给定数据集 $\left \{ \boldsymbol {x,t} \right \}$ 上的条件概率分布，可表示为 $p(\boldsymbol w| \boldsymbol{x,t})$ ，也称作 $\boldsymbol w$ 的后验概率 (posterior distribution)，与似然概率 $p(\boldsymbol t|\boldsymbol{w,x})$ 相同的是，我们总是将概率的条件项假设为定值。使⽤贝叶斯定理 (Bayes Theorem)， $\boldsymbol w$ 的后验概率正⽐于先验分布和似然函数的乘积。

$$p(\boldsymbol w| \boldsymbol{x},\boldsymbol{t},\alpha,\beta)\propto p(\boldsymbol t|\boldsymbol{w},\boldsymbol{x},\beta)p(\boldsymbol w|\alpha)\tag 9$$
为了最大化后验概率分布确定 $\boldsymbol w$ ，同样将高斯分布公式代入并取负对数，可得最大化后验概率就是最小化下式

$$\frac{\beta}{2} \sum_{n=1}^{N} \left \{ y(x_n,\boldsymbol{w})-t_n \right \}^2+\frac{\alpha}{2}\boldsymbol{w}^T\boldsymbol{w}\tag{10}$$
第二项 $\frac{\alpha}{2}\boldsymbol{w}^T\boldsymbol{w}$ 也称作 $L_2$ 正则项 ($L_2$  regularization )，具有防止过拟合 (overfitting) 的作用，这些将在后续章节中详细讨论，下一篇文章将讲述误差函数 $E(\boldsymbol{w})$ 的求解方式，我们只考虑最大似然项，即针对 $\boldsymbol w$ 的最小平方和函数。