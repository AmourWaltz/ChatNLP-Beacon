# [最小均方差 Minimum Square Error](./ch1_linear_regression/1.4_minimum_square_error.md)

> [1.1 多项式拟合 Polynomial Fitting](./ch1_linear_regression/1.1_polynomial_fitting.md) </br>
> [1.2 线性基函数模型 Linear Basis Function Model](./ch1_linear_regression/1.2_linear_basis_function_model.md) </br>
> [1.3 最大似然估计 Maximum Likelihoood Estimation](./ch1_linear_regression/1.3_maximum_likelihoood_estimation.md) </br>
> [1.4 最小均方差 Minimum Square Error](./ch1_linear_regression/1.4_minimum_square_error.md) </br>
> [1.5 梯度下降法 Gradient Desent](./ch1_linear_regression/1.5_gradient_desent.md) </br>
> [1.6 解析法 Analytic Method](./ch1_linear_regression/1.6_analytic_method.md) </br>

上回说到，我们利用线性回归模型进行拟合，需要根据训练数据调节参数 $\boldsymbol{w}$ 的值，使得对于任意训练数据 $\boldsymbol{x}_j$ ，模型的输出 $y(\boldsymbol{x}_j$, $\boldsymbol{w})$ 更加接近目标值 $t_j$ ，同时最小化代价函数 （或误差函数）$E(\boldsymbol{w})=\frac {1}{2}\sum_{n=1}^{N}[y(x_n,\boldsymbol{w})-t_n]^2$ 使之无限趋近于0。由于误差函数是系数 $\boldsymbol{w}$ 的二次函数（线性回归 $y(\boldsymbol{x}_j, \boldsymbol{w})$ 本身定义就是针对 $\boldsymbol{w}$ 的线性函数），所以其导数是关于 $\boldsymbol{w}$ 的线性函数，误差函数的最小值有唯一最优解 $\boldsymbol{w^\star}$ ，线性回归问题也可以看作是针对误差函数的优化问题。但由于求解系数 $\boldsymbol{w}$ 通常是在一整个大数据集上进行的，我们很难得到精确的 $\boldsymbol{w^\star}$ ，最直观的方法就是逐一考虑每一个数据点 $\boldsymbol{x}_j$ 来逼近 $\boldsymbol{w^\star}$ 。
这种求解方法也称作顺序算法 (sequential algorithm)（或在线算 (on-line algorithm)），通过迭代每次只考虑⼀个数据点，模型的参数在每观测到数据点之后进⾏更新（实际上一般不会在每个点计算后都更新一次，这样效率极低且误差函数往往震荡不收敛。一般都是在小批量操作的），在迭代过程中通常梯度下降 (gradient descent) 算法来逐步优化求解 $\boldsymbol{w^\star}$ 。

为了能找到使误差函数 $E(\boldsymbol{w})$ 最小的参数 $\boldsymbol{w}$ ，我们对 $\boldsymbol{w}$ 的初值先做出一定假设（比如初始化为0），然后在利用梯度下降算法不断更新 $\boldsymbol{w}$ 使之收敛至某处可以最小化 $E(\boldsymbol{w})$ ，如下

$$\boldsymbol w^{(\tau+1)}=\boldsymbol w^{(\tau)}-\eta\nabla E(\boldsymbol w)\tag 1$$
其中 $\tau$ 是迭代次数， $\eta$ 被称作学习率，是我们需要手动调节的超参数 (hyperparameter)，会直接影响到函数能否收敛，需要反复斟酌选定。梯度下降算法通过反复迭代使权值向量 $\boldsymbol w$ 都会沿着误差函数 $E(\boldsymbol{w})$ 下降速度最快的⽅向移动，因此这种⽅法被称为梯度下降法或最速下降法 (steepest descent)。假设线性回归函数形式为 $y(\boldsymbol{x,w})=\boldsymbol{w}^T\phi(\boldsymbol{x})$ ，方便起见我们令 $\phi(\boldsymbol x)=\boldsymbol x$ ，对于 $y$ 的平⽅和误差函数，有

$$\boldsymbol w^{(\tau+1)}=\boldsymbol w^{(\tau)}-\eta(t_n-\boldsymbol w^{(\tau)T}\boldsymbol x_n)\tag 2$$ 
这被称为最小均方 (least mean squares, LMS) 或 Widrow-Hoff 算法。这种⽅法在直觉上看很合理，预测和目标之差 $t-\mathbf w^{(\tau)T}\boldsymbol x_n$ 是参数 $\boldsymbol{w}$ 更新幅值的一项因子，包含梯度方向和幅值信息，因此，当预测值与真实目标比较接近时， $\boldsymbol{w}$ 变化幅度也会较小，反之亦然。

## 几何描述

我们考虑⼀个 $N$ 维空间，它的坐标轴由 $t_n$ 给出， 即 $\boldsymbol t=\left \{ t_1,t_2,...t_N \right \}^T$ 是这个空间中的⼀个向量。 每个在 $N$ 个数据点处估计的基函数 $\phi_j(\boldsymbol x_n)$ 也可以表示为这个空间中的⼀个向量， 记作 $\varphi_j 。 \varphi_j$ 对应于 $\Phi$ 的第 $j$ 列，⽽ $\phi(\boldsymbol x_n)$ 对应于 $\Phi$ 的第 $n$ ⾏。 如果基函数的数量 $M$ ⼩于数据点的数量 $N$ (很容易联想特征数小于数据点个数，未知参数才有唯一解)，那么 $M$ 个向量 $\varphi_j$ 将会张成⼀个 $M$ 维的⼦空间 $\mathcal{S}$  。我们定义 $\boldsymbol y=\left \{ y(\boldsymbol x_1,\boldsymbol w),y(\boldsymbol x_2,\boldsymbol w),...,y(\boldsymbol x_N,\boldsymbol w) \right \}^T$ 是⼀个 $N$ 维向量。由于 $\boldsymbol y$ 是 $M$ 个向量 $\varphi_j$ 的任意线性组合，因此它可以位于 $M$ 维⼦空间的任何位置。这样，平⽅和误差函数就等于 $\boldsymbol y$ 和 $\boldsymbol t$ 之间的欧⽒平方距离。 因此， $\boldsymbol w$ 的最⼩平⽅解对应位于⼦空间 $\mathcal S$ 的与 $\boldsymbol t$ 最近的 $\boldsymbol y$ 的选择。直观来看，根据下图，我们很容易猜想这个解对应于 $\boldsymbol t$ 在⼦空间 $\mathcal S$ 上的正交投影。 事实上确实是这样。 假设 $\boldsymbol y^{\mathcal {(S)}}$ （下图中的 $\boldsymbol y$ ，此处重新定义向量是针对原书做区分，原书中已经定义了 $\boldsymbol y$ 是一个 $N$ 维向量此处又出现在⼦空间 $\mathcal S$ 上容易产生误解）是子空间 $\mathcal S$ 上 $\varphi_1,\varphi_2$ 在参数为 $\boldsymbol w_{ML}$ 时的线性组合，只要证明它的表达式为 $\boldsymbol t$ 的正交投影即可。

<div align=center>
<img src="images/1_4_mse1.png" width="80%"/>
</div>
