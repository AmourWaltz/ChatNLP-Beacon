# [多项式拟合 Polynomial Fitting](./ch1_linear_regression/1.1_polynomial_fitting.md)

> [1.1 多项式拟合 Polynomial Fitting](./ch1_linear_regression/1.1_polynomial_fitting.md) </br>
> [1.2 线性基函数模型 Linear Basis Function Model](./ch1_linear_regression/1.2_linear_basis_function_model.md) </br>
> [1.3 最大似然估计 Maximum Likelihoood Estimation](./ch1_linear_regression/1.3_maximum_likelihoood_estimation.md) </br>
> [1.4 最小均方差 Minimum Square Error](./ch1_linear_regression/1.4_minimum_square_error.md) </br>
> [1.5 梯度下降法 Gradient Desent](./ch1_linear_regression/1.5_gradient_desent.md) </br>
> [1.6 解析法 Analytic Method](./ch1_linear_regression/1.6_analytic_method.md) </br>

在大部分实际应用中，目标变量很难与输入变量呈线性关系，仅使用上述线性方程 $ y(\boldsymbol{x,w}) $ 就想完美拟合出输入与目标的关系难乎其难，因此我们试图使用更一般的多项式函数进行拟合，此处将上述线性回归函数简化为单变量函数 $ y(x, \boldsymbol{w})=w_0+w\cdot x $，即认为原始输入变量 $ \boldsymbol{x} $ 只有一个维度的特征，其多项式回归函数形式为

$$y(x, \boldsymbol{w})=w_0+w_1\cdot x + w_2\cdot x^2 + \cdot \cdot \cdot +w_K\cdot x^K=\sum_{n=0}^{K} w_j\cdot x^j\tag1$$ 
其中 $ K $ 是多项式的阶数 (order)，多项式系数 $ w_0, w_1, ..., w_K $ 整体记作向量 $ \boldsymbol{w} $。多项式函数是一类典型的线性回归模型，其线性关系实际上是针对系数 $ \boldsymbol{w} $ 而并非 $ \boldsymbol{x} $，具备基本的线性特性

$$f(a\boldsymbol{w_1})=af(\boldsymbol{w_1}),    f(\boldsymbol{w_1}+\boldsymbol{w_2})=f(\boldsymbol{w_1})+f(\boldsymbol{w_2})\tag2 $$
其中 $ \boldsymbol{w_1},\boldsymbol{w_2} $ 为两组可能的参数向量。

使用多项式函数进行曲线拟合不无道理，根据泰勒公式 (Taylor's Formula)，对于任意连续函数 $ f(x) $，如果$ f(x) $在 $ x_0 $ 处具有任意阶导数，其泰勒展开公式为

$$f(x)=f(x_0)+(x-x_0){f}'(x_0)+ (x-x_0)^2 \frac{{f}''(x_0)}{2!}+···=\sum_{n=0}^{\infty }(x-x_0)^n\frac{{f}^{(n)}(x_0)}{2!} =\sum_{n=0}^{\infty} c_nx^{n}\tag3$$
正好可以简化为多项式函数的形式，尤其对于自变量只在小范围区间上定义的模型，多项式函数可以近乎完美的拟合曲线；但是多项式函数的⼀个局限在于它们是输⼊变量的全局函数，其形式也并不唯一，在输⼊空间一个区域引入新的训练数据往往可能影响所有其他区域的数据拟合结果。对此我们把输⼊空间切分成若⼲个区域，然后在每个区域⽤不同的多项式函数拟合，这种函数称做样条函数 (spline function)。

多项式函数系数的值可以通过调整函数拟合训练数据的⽅式确定，一般通过最小化误差函数 (error function) 实现。⼀个简单且应⽤⼴泛的误差函数是计算每个数据点 $ x_n $ 的预测值 $ y(x_n,\boldsymbol{w}) $ 与⽬标值 $ t_n $ 之差的最小平⽅和 (least squares)，其形式为

$$E(\boldsymbol{w})=\frac {1}{2}\sum_{n=1}^{N}[y(x_n,\boldsymbol{w})-t_n]^2\tag4$$
其中 $ E(\boldsymbol{w}) $ 也称作代价函数 (cost function)，引入因⼦ $ \frac {1}{2} $ 是为了简化一阶导数形式。