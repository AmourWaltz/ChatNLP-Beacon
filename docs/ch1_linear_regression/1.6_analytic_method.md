# [解析法 Analytic Method](./ch1_linear_regression/1.6_analytic_method.md)

> [1.1 多项式拟合 Polynomial Fitting](./ch1_linear_regression/1.1_polynomial_fitting.md) </br>
> [1.2 线性基函数模型 Linear Basis Function Model](./ch1_linear_regression/1.2_linear_basis_function_model.md) </br>
> [1.3 最大似然估计 Maximum Likelihoood Estimation](./ch1_linear_regression/1.3_maximum_likelihoood_estimation.md) </br>
> [1.4 最小均方差 Minimum Square Error](./ch1_linear_regression/1.4_minimum_square_error.md) </br>
> [1.5 梯度下降法 Gradient Desent](./ch1_linear_regression/1.5_gradient_desent.md) </br>
> [1.6 解析法 Analytic Method](./ch1_linear_regression/1.6_analytic_method.md) </br>

我们可以利用梯度下降法来求解最小平方和函数，回忆第一节所定义的平方和误差函数 $E(\boldsymbol{w})=\frac {1}{2}\sum_{n=1}^{N}[y(x_n,\boldsymbol{w})-t_n]^2$ ，也是似然函数，我们可以用最大似然的方法确定 $\boldsymbol w$ 和 $\beta$ ，在条件⾼斯噪声分布的情况下，采用线性基函数的形式，线性函数的对数似然函数如下

$$\ln_{}{p(\boldsymbol{t} |\boldsymbol{x},\boldsymbol{w},\beta) } =-\frac{\beta}{2} \sum_{n=1}^{N} \left \{ t_n - \boldsymbol{w}^T\phi (\boldsymbol x_n) \right \}^2+\frac{N}{2}\ln_{}{\beta }-\frac{N}{2}\ln_{}{2\pi }\tag1$$
线性模型的似然函数的最⼤化等价于平⽅和误差函数的最⼩化，我们在第二篇文章提及，对数似然函数是关于 $\boldsymbol{w}$ 的二次函数，所以似然函数有唯一最优解 $\boldsymbol{w}_{ML}$ ，前文中我们是从微观的角度将训练集中的数据依次代入计算 $\boldsymbol{w}$ ，通过梯度下降算法用迭代的形式逼近 $\boldsymbol{w}_{ML}$。本文将使用一种更为直观，整体的方法，首先对数似然函数的梯度为

$$\nabla \ln_{}{p(\boldsymbol{t} |\boldsymbol{x},\boldsymbol{w},\beta) } = \beta\sum_{n=1}^{N} \left \{t_n - \boldsymbol{w}^T\phi (\boldsymbol x_n)\right \}\phi (\boldsymbol x_n)\tag 2$$
令对数似然函数的梯度为 0 求解 $\boldsymbol{w}_{ML}$ ，可得

$$\boldsymbol{w}_{ML} = ( \Phi^T \Phi)^{-1}\Phi^T\boldsymbol t\tag 3$$
这就是使用解析法 (analytic method) 对方程直接求解，也称为最小平方问题的规范方程 (normal equation)。这⾥ $\Phi$ 是⼀个 $N\times M$ 的矩阵，被称为设计矩阵 (design matrix)，它的元素为 $\Phi_{i,j}=\phi_j(\boldsymbol x_i)$ ，即

$$\Phi=\begin{pmatrix}  \phi_0 (\boldsymbol x_1) & \phi_1 (\boldsymbol x_1) & \cdot \cdot \cdot  & \phi_{M-1} (\boldsymbol x_1) \\  \phi_0 (\boldsymbol x_2) & \phi_1 (\boldsymbol x_2) & \cdot \cdot \cdot  & \phi_{M-1} (\boldsymbol x_2) \\  \vdots  & \vdots  & \ddots  & \vdots \\  \phi_0 (\boldsymbol x_N) & \phi_1 (\boldsymbol x_N) & \cdot \cdot \cdot  & \phi_{M-1} (\boldsymbol x_N)  \end{pmatrix}$$
量 $\Phi^{\dagger }\equiv (\Phi^T\Phi)^{-1}\Phi^T$ 被称为矩阵 $\Phi$ 的 Moore-Penrose 伪逆矩阵 (pseudo-inverse matrix)。 它可以被看成逆矩阵的概念对于⾮⽅阵的矩阵的推⼴。实际上，如果 \Phi 是⽅阵且可逆，那么使⽤性质 ($\boldsymbol {AB})^{-1}=\boldsymbol B^{-1} \boldsymbol A^{-1}$ ，我们可以看到 $\Phi ^\dagger \equiv \Phi ^{-1}$ 。在实际应⽤中，当 ($\Phi^T\Phi)^{-1}$ 接近奇异矩阵时，直接求解规范⽅程会导致数值计算上的困难。特别地，当两个或者更多的基函数向量共线（矩阵 $\Phi$ 的列）或者接近共线时，最终的参数值会相当⼤。这种数值计算上的困难可以通过奇异值分解 (singular value decomposition) 的⽅法解决。注意，正则项的添加确保了矩阵是⾮奇异的。

注意此处关于基函数的写法，实际上 (12) 式中 $\phi (\boldsymbol x_n) = \left \{ \boldsymbol \phi_1(\boldsymbol x_n), \boldsymbol \phi_2(\boldsymbol x_n),...,\boldsymbol \phi_{M-1}(\boldsymbol x_n) \right \}$ ，参数 $\boldsymbol w$ 的维度是 $M$ ，在第一篇文章节提到，线性基函数变换的本质还是特征提取，此处直接对基函数变换之后的特征进行线性变换，所以我们忽略了数据点 $\boldsymbol x_n$ 的原始维度，而 CS229 讲义中是对原始数据  $\boldsymbol x_n$ 进行线性变换，所以写法上略有不同，但原理是一样的，此处的 $\Phi$ 相当于 CS229 讲义中的 X ，是一个 $N\times M$ 的矩阵，横向是将 (12) 式中的求和步骤改为将所有训练数据点合并在矩阵内，纵向是每个数据点的特征，如果是直接进行线性变换就是原始数据点的维度，如果进行过基函数变换就是基函数的数量。
现在，我们可以更加深刻地认识偏置参数 $w_0$ 。如果我们显式地写出偏置参数，那么误差函数变为

$$E(\boldsymbol{w})=\frac {1}{2}\sum_{n=1}^{N}[t_n-w_0-\sum_{j=1}^{M-1}w_j\phi_j(\boldsymbol x_n)]^2\tag 4$$
令关于 $w_0$ 的导数等于零，解出 $w_0$ ，可得

$$w_0=\frac {1}{N}\sum_{n=1}^{N}t_n-\sum_{j=1}^{M-1}w_j\cdot\sum_{n=1}^{N}\phi_j(\boldsymbol x_n)\tag 5$$
可以看出偏置 $w_0$ 补偿了训练数据⽬标值的平均值 $\frac {1}{N}\sum_{n=1}^{N}t_n$ 与基函数的值的平均值的加权求和 $\sum_{n=1}^{N}\phi_j(\boldsymbol x_n)$ 之间的差。
回归第一篇文章内容，我们也可以关于噪声精度参数 $\beta$ 最⼤化似然函数

$$\frac{1}{\beta_{ML}} =\frac{1}{N} \sum_{n=1}^{N} \left \{ t_n-\boldsymbol{w}_{ML}^T\phi(\boldsymbol x_n) \right \}^2\tag 6$$
因此我们看到噪声精度的倒数由⽬标值在回归函数周围的残留⽅差 (residual variance) 给出。

