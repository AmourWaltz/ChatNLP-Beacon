# [梯度下降法 Gradient Desent](./ch1_linear_regression/1.5_gradient_desent.md)

> [1.1 多项式拟合 Polynomial Fitting](./ch1_linear_regression/1.1_polynomial_fitting.md) </br>
> [1.2 线性基函数模型 Linear Basis Function Model](./ch1_linear_regression/1.2_linear_basis_function_model.md) </br>
> [1.3 最大似然估计 Maximum Likelihoood Estimation](./ch1_linear_regression/1.3_maximum_likelihoood_estimation.md) </br>
> [1.4 最小均方差 Minimum Square Error](./ch1_linear_regression/1.4_minimum_square_error.md) </br>
> [1.5 梯度下降法 Gradient Desent](./ch1_linear_regression/1.5_gradient_desent.md) </br>
> [1.6 解析法 Analytic Method](./ch1_linear_regression/1.6_analytic_method.md) </br>

梯度下降算法使⽤梯度信息，权值参数 $\boldsymbol{w}$ 通过迭代的方式，每次在误差函数 $E(\boldsymbol{w})$ 关于 $\boldsymbol{w}$ 负梯度⽅向上通过⼀次⼩的移动进行更新。误差函数是关于训练集定义的，因此计算 $\nabla E(\boldsymbol{w})$  ，每⼀步都需要处理整个训练集，也称作批量梯度下降 (batch gradient descent)，如下

$$\boldsymbol w^{(\tau+1)}=\boldsymbol w^{(\tau)}-\eta \frac{1}{N}\sum_{n=1}^{N}(t_n-\boldsymbol w^{(\tau)T}\boldsymbol x_n)\tag 1$$
需要注意的是，虽然梯度下降通常可能会受到局部极小值的影响，但我们在此处针对的优化问题 $E(\boldsymbol{w})$ 是一个关于 $\boldsymbol{w}$ 的二次凸函数 (convex quadratic function)，因此它只存在一个全局极小值 (global minimum) 且没有局部最小 (local minimum)。如下图所示，椭圆表示二次型函数轮廓的轮廓采样。蓝色轨迹表示参数通过梯度下降算法的连续过程

<div align=center>
<img src="images/1_5_gd1.png" width="80%"/>
</div>

对于非线性的情形，误差函数 $E(\boldsymbol{w})$ 可能存在多个局部最优，为了能找到⼀个⾜够好的极⼩值，可能有必要多次运⾏梯度下降算法，每次都随机选⽤不同的起始点，然后在⼀个独⽴的数据集上对⽐最终性能。
实际上批量梯度下降法在大数据集使用时效率很低，每一步都需要计算全部数据的梯度值，运算开销很高。然⽽，梯度下降法有另⼀个版本，与批量梯度下降算法不同，我们每一步更新都是基于⼀个独⽴观测数据的误差函数，这种算法也称作随机梯度下降 (stochastic gradient descent) 或顺序梯度下降 (sequential gradient descent, Bishop PRML) 或递增梯度下降 (incremental gradient descent, CS229)，使得权值参数的更新每次只依赖于⼀个数据点，即

$$\boldsymbol w^{(\tau+1)}=\boldsymbol w^{(\tau)}-\eta(t_n-\boldsymbol w^{(\tau)T}\boldsymbol x_n)\tag 2$$
与批量梯度下降相⽐，随机梯度下降的⼀个优点是可以更加⾼效地处理数据中的冗余性。考虑⼀种极端的情形：给定⼀个数据集，我们将每个数据点都复制⼀次，从⽽将数据集的规模翻倍，但这仅仅把误差函数乘以⼀个因⼦2，等价于使⽤原始的误差函数。批量梯度下降法必须付出两倍的计算量来计算误差函数的梯度，⽽在随机梯度下降法则不受影响。

随机梯度下降另⼀个特点是便于逃离局部极⼩值点，因为整个数据集误差函数的驻点通常不会是单一数据点误差函数的驻点，然而，也正因如此，我们几乎永远无法得到关于整个数据集的参数最优解 $\boldsymbol w^\star$ ，因为我们每次计算的都是单一数据点误差函数关于 $\boldsymbol w$ 的梯度。此外，随机梯度下降法的参数更新过于频繁，除了迭代次数会骤增，权值参数的更新过程也会不断震荡很难收敛。折中的⽅法是，每次更新依赖于一组独立观测的数据点，实现小批量数据上的随机梯度下降 (minibatch stochastic gradient descent)。

## 二次函数曲率

有时我们需要计算输入输出均为向量的函数的所有偏导数（可以理解为多目标的回归问题或分类问题），包含所有这样的偏导数的矩阵被称为 Jacobian 矩阵。具体来说，如果我们有一个函数映射 $f(\boldsymbol x):\mathbb{R}^m\rightarrow\mathbb{R}^n$ ， $f$ 的 Jacobian 矩阵 $\boldsymbol J \in\mathbb{R}^{n\times m}$ 定义为 $J_{i,j}=\frac{\partial }{\partial x_j} {f(\boldsymbol x)}_i$ 。
有时，我们也对导数的导数感兴趣，即二阶导数 (second derivative)，一维情况下，我们将二阶导数记作 $\frac{\partial^2 }{\partial x^2} {f(x)}$ ，二阶导数表示一阶导数如何随着输入的变化而改变，即基于梯度信息的下降步骤是否会产生如我们预期的那样的改善。二阶导数是对曲率 (curvature) 的衡量。

延续上述篇幅的所使用的二次误差函数 $E(\boldsymbol w)$ ，真实的误差函数形式往往要复杂许多，但利用泰勒展开我们可以在小范围区间用多项式函数拟合绝大部分未知函数，而随机梯度下降的更新也是在微观步骤上更新的，因此对于连续可导的误差函数 $E(\boldsymbol w)$ ，我们同样可以把它当做一个样条函数（参见第一篇文章），每一处都当做一个多项式函数，为简便起见我们只考虑到二阶泰勒展开级数，也就是使用二次函数进行局部拟合，此时误差函数就可以看做由无数个不同的二次函数的微小区间所组成的连续且处处可导的函数。
取二次函数进行拟合也是为了更方便的观察二阶导数，我们使用沿负梯度方向大小为的 $\epsilon$ 下降步（类似上文梯度下降算法中的学习率 $\eta$ ，因为梯度下降算法目标与预测值之差也会包含一定下降幅度信息，而此处下降步包含了所有梯度下降的幅度信息）。当该梯度是 1 时，代价函数将下降 $\epsilon$ 。 如果二阶导数是负的 (negative curvature)，函数曲线向上凸出，代价函数将下降的比 $\epsilon$ 多；如果二阶导数为 0 (no curvature)，那就没有曲率，仅用梯度就可以预测它的值；如果二阶导数是正的 (positive curvature)，函数曲线是向下凸出，代价函数将下降的比 $\epsilon$ 少。下图依次反应了三种情况的曲率如何影响基于梯度的预测值与真实误差函数值的关系。

<div align=center>
<img src="images/1_5_gd2.png" width="80%"/>
</div>

## Hessian 特征值

当函数具有多维输入输出时，二阶导数也有很多。我们可以将这些导数合并成一个矩阵，称为 Hessian 矩阵。Hessian 矩阵 $\boldsymbol H (f)(\boldsymbol x)$ 定义为

$$\boldsymbol H (f)(\boldsymbol x )_{i,j}=\frac{\partial^2 }{\partial x_i \partial x_j} {f(\boldsymbol x)}=\frac{\partial^2 }{\partial x_j \partial x_i} {f(\boldsymbol x)}\tag 3$$
其中二阶导数项表示 $f$ 的一阶导数（关于 $x_j$ ）关于 $x_i$ 的导数，Hessian 等价于梯度的 Jacobian 矩阵，微分算子在任何二阶偏导连续的点处可交换，也就是它们的顺序可以互换。这意味着 $\boldsymbol H_{i,j}=\boldsymbol H_{j,i}$ ，因此 Hessian 矩阵在这些点上是对称的。因为 Hessian 矩阵是实对称的，我们可以将其分解成一组实特征值和一组特征向量的正交基。在特定方向 $\boldsymbol d$ 上的二阶导数可以写成 $\boldsymbol d^{T}\boldsymbol {Hd}$ 。当 $\boldsymbol d$ 是 $\boldsymbol H$ 的一个特征向量时，这个方向的二阶导数就是对应的特征值。对于其他的方向 $\boldsymbol d$ ，方向二阶导数是所有特征值的加权平均，权重在 0 和 1 之间， 且与 $\boldsymbol d$ 夹角越小的特征向量的权重越大。最大特征值确定最大二阶导数，最小特征值确定最小二阶导数。
我们可以通过（方向）二阶导数预期一个梯度下降步骤能表现得多好。我们在 当前点 $\boldsymbol {x}^{(0)}$ 处作函数 $f(\boldsymbol x)$ 的近似二阶泰勒级数：

$$f(\boldsymbol x)\approx f(\boldsymbol x^{(0)})+(\boldsymbol x-\boldsymbol x^{(0)})^T\boldsymbol g+\frac{1}{2}(\boldsymbol x-\boldsymbol x^{(0)})^T\boldsymbol H(\boldsymbol x-\boldsymbol x^{(0)})\tag 4$$
其中 $\boldsymbol g$ 是梯度， $\boldsymbol H$ 是 $\boldsymbol {x}^{(0)}$ 点的 Hessian。如果我们使用学习率 $\eta$ ， 那么新的点 $\boldsymbol x$ 将会是 $\boldsymbol x^{(0)}-\eta\boldsymbol g$ 。 代入上述近似，可得

$$f(\boldsymbol x^{(0)}-\eta\boldsymbol g)\approx f(\boldsymbol x^{(0)})+-\eta\boldsymbol g^T\boldsymbol g+\frac{1}{2} \eta^2 \boldsymbol g^T\boldsymbol H\boldsymbol g\tag 5$$
其中有 3 项：函数的原始值、函数斜率导致的预期改善、函数曲率导致的校正。当最后一项太大时，梯度下降实际上是可能向上移动的。当 $\boldsymbol g^T\boldsymbol H\boldsymbol g$ 为零或负时，近似的泰勒级数表明增加 $\eta$ 将永远使 $f$ 下降。在实践中，泰勒级数不会在 $\eta$ 大的时候也保持准确，因为我们对误差函数的近似二次拟合都是微小区间的，学习率过大很可能就到另一个二次函数的区间了，因此在这种情况下我们必须采取更启发式的选择。当 $\boldsymbol g^T\boldsymbol H\boldsymbol g$ 为正时，通过计算可得，使近似泰勒级数下降最多的最优学习率为

$$\eta^\star=\frac{\boldsymbol g^T\boldsymbol g}{\boldsymbol g^T\boldsymbol H\boldsymbol g}\tag 6$$
最坏的情况下， $\boldsymbol g$ 与 $\boldsymbol H$ 最大特征值 $\lambda_{max}$ 对应的特征向量对齐，则最优步长是 $\frac {1}{\lambda_{max}}$ 。 我们要最小化的误差函数能用二次函数很好地近似的情况下，Hessian 的特征值决定了学习率的量级。

## 二阶导数测试

二阶导数还可以被用于确定一个临界点是否是局部极大点、局部极小点或鞍点。在临界点处 ${f}' (x)=0$ 。而 ${f}''(x) > 0$ 意味着 ${f}' (x)$ 会随着我们移向右边而增加，移向左边而减小，也就是 ${f}' (x-\delta)<0 和 {f}' (x-\delta)>0$ 对足够小的 $\delta$ 成立。因此我们得出结论，当 ${f}' (x)=0$ 且 ${f}''(x) > 0$ 时， $x$ 是一个局部极小点。同样，当 ${f}' (x)=0$ 且 ${f}''(x) <0$ 时， $x$ 是一个局部极大值点。这就是所谓的二阶导数测试 (second derivative test)。不幸的是，当 ${f}''(x) = 0$ 时测试是不确定的。在这种情况下， $x$ 可以是一个鞍点或平坦区域的一部分。

在多维情况下，我们需要检测函数的所有二阶导数。利用 Hessian 的特征值分解，我们可以将二阶导数测试扩展到多维情况。在临界点处 $\nabla_\boldsymbol x  f(\boldsymbol x)=0$ ，我们通过检测 Hessian 特征值来判断该临界点是一个局部极大点、局部极小点还是鞍点。 当 Hessian 是正定的（所有特征值都是正的），则该临界点是局部极小点。同样的，当 Hessian 是负定的（所有特征值都是负的），这个点就是局部极大点。在多维情况下，我们可以找到确定该点是否为鞍点的迹象。如果 Hessian 的特征值中至少一个是正的且至少一个是负的，那么 $x$ 是 $f$ 某个横截面的局部极大点，却是另一个横截面的局部极小点，典型例子就是鞍点。最后，多维二阶导数测试可能像单变量版本那样是不确定的。当所有非零特征值是同号的且至少有一个特征值是 0 时，这个检测就是不确定的。这是因为单变量的二阶导数测试在零特征值对应的横截面上是不确定的。

多维情况下， 单个点处每个方向上的二阶导数是不同。Hessian 的条件数 (condition number) 衡量这些二阶导数的变化范围。当 Hessian 的条件数很差时，梯度下降法也会表现得很差。这是因为一个方向上的导数增加得很快，而在另一个方向上增加得很慢。梯度下降不知道导数的这种变化，所以它不知道应该优先探索导数长期为负的方向，如下图所示。为了避免冲过最小而向具有较强正曲率的方向，防止发生震荡，我们需要将学习率设置较小，然而这也导致了更新步长太小，在其他较小曲率的方向上进展不明显。

<div align=center>
<img src="images/1_5_gd3.png" width="80%"/>
</div>

## 牛顿法

上图中，红线表示梯度下降的路径。这个非常细长的二次函数类似一个长峡谷。梯度下降把时间浪费于在峡谷壁反复下降，因为它们是最陡峭的特征。由于步长有点大，有超过函数底部的趋势，因此需要在下一次迭代时在对面的峡谷壁下降。与指向该方向的特征向量对应的 Hessian 的大的正特征值表示该方向上的导数值仍然较大，因此基于 Hessian 的优化算法可以预测，在此情况下最陡峭方向实际上不是有前途的搜索方向。

我们可以使用 Hessian 矩阵的信息来指导搜索，以解决这个问题。其中最简单的方法是牛顿法 (Newton’s method)。仅使用梯度信息的优化算法被称为一阶优化算法 (ﬁrst-order optimization algorithms)，如梯度下降。使用 Hessian 矩阵的优化算法被称为二阶最优化算法 (second-order optimization algorithms)，如牛顿法。牛顿法基于一个二阶泰勒展开来近似 $\boldsymbol {x}^{(0)}$ 附近的 $f(\boldsymbol x)$ ：

$$f(\boldsymbol x)\approx f(\boldsymbol x^{(0)})+(\boldsymbol x-\boldsymbol x^{(0)})^T\nabla_\boldsymbol x  f(\boldsymbol x^{(0)})+\frac{1}{2}(\boldsymbol x-\boldsymbol x^{(0)})^T\boldsymbol H(f)(\boldsymbol x^{(0)})(\boldsymbol x-\boldsymbol x^{(0)})\tag 7$$
计算这个函数的临界点，令 ${f}' (\boldsymbol x)=0$ ，得

$$\boldsymbol x^\star=\boldsymbol x^{(0)}-\boldsymbol H(f)(\boldsymbol x^{(0)}) ^{-1}\nabla_\boldsymbol x  f(\boldsymbol x^{(0)})\tag 8$$
当 $f$ 是一个正定二次函数时，牛顿法只要应用一次式就能直接跳到函数的最小点。如果 $f$ 能在局部近似为正定二次，牛顿法则需要多次迭代。迭代地更新近似函数和跳到近似函数的最小点可以比梯度下降更快地到达临界点。这在接近局部极小点时是一个特别有用的性质，但是在鞍点附近是有害的。当附近的临界点是最小点（Hessian 的所有特征值都是正的）时牛顿法才适用，而梯度下降不会被鞍点吸引。