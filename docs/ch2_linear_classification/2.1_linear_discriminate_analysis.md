# [线性判别分析 Linear Discriminate Analysis](./ch2_linear_classification/2.1_linear_discriminate_analysis.md)

> [2.1 线性判别分析 Linear Discriminate Analysis](./ch2_linear_classification/2.1_linear_discriminate_analysis.md) </br>
> [2.2 Fisher分类器 Fisher Classifier](./ch2_linear_classification/2.2_fisher_classifier.md) </br>
> [2.3 感知器算法 Perceptron Algorithm](./ch2_linear_classification/2.3_perceptron_algorithm.md) </br>
> [2.4 判别式 Logistic 回归 Discrminate Logistic Regression](./ch2_linear_classification/2.4_discriminate_logistic_regression.md) </br>
> [2.5 生成式 Logistic 回归 Generative Logistic Regression](./ch2_linear_classification/2.5_generative_logistic_regression.md) </br>
> [2.6 广义线性模型 Generalized Linear Model](./ch2_linear_classification/2.6_generalized_linear_model.md) </br>

与线性回归相似，一种最简单的模型就是直接用一个线性函数进行分类，首先考虑二分类的情形，我们构造一个输入向量的线性函数 $y(\boldsymbol x)=\boldsymbol w^T\boldsymbol x+w_0$ ，对于输入向量 $\boldsymbol x$ ，如果 $y(\boldsymbol x)\geq0$ ，那么就被分到 $C_1$ 类，否则分到 $C_2$ 类。此时决策面由 $y(\boldsymbol x)=0$ 确定。对于任意两个在决策面上的点 $\boldsymbol x_A$ 和 $\boldsymbol x_B$ ，有 $\boldsymbol w(\boldsymbol x_A-\boldsymbol x_B)=0$ ，所以向量 $\boldsymbol w$ 正交于决策面。对于任意一点 $\boldsymbol x$ ，在 $\boldsymbol w$ 方向上的投影代表了原点到决策面的垂直距离，即

$$\frac{\boldsymbol w^T\boldsymbol x}{\left \| \boldsymbol w \right \| }=-\frac{w_0}{\left \| \boldsymbol w \right \| }\tag 1$$
可见 $\boldsymbol w$ 和 $w_0$ 分别决定了决策面的方向和位置。对于任意一点 $\boldsymbol x$ ，我们将其投影至决策面上的一点 $\boldsymbol x_\bot$ ，这样我们就可以将其写成两个向量之和的形式

$$\boldsymbol x=\boldsymbol x_\bot+r\frac{\boldsymbol w}{\left \| \boldsymbol w \right \| }\tag 2$$
在两边同乘 $\boldsymbol w$ 并加上 $w_0$ ，由于 $\boldsymbol x_\bot$ 在决策面上，因此可以得到 $r=\frac{f(\boldsymbol x)}{\left \| \boldsymbol w \right \| }$ ，如下图所示。

<div align=center>
<img src="images/2_1_lda1.png" width="80%"/>
</div>

关于多分类的线性判别函数，我们可以引入一个由 $K$ 个线性判别函数组成的 $K$ 类判别函数 $y_k(\boldsymbol x)=\boldsymbol w_k^T\boldsymbol x+w_k0$ ，对于任意一点 $\boldsymbol x$ ，如果对于所有的 $i\ne j$ 都有 $y_k(\boldsymbol x)>y_j(\boldsymbol x)$ ，那么就把它分到 $C_k$ ，此时 $C_k$ 与 $C_j$ 的决策面为 $y_k(\boldsymbol x)=y_j(\boldsymbol x)$ ，对应于一个 $D-1$ 维超平面，形式为 $(\boldsymbol w_k-\boldsymbol w_j)^T\boldsymbol x+(w_{k0}-w_{j0})=0$ ，具有二分类决策面类似的性质。并且这样的决策区域都是单连通且凸的。
考虑决策区域 $\mathcal R_k$ 内任意两点 $\boldsymbol x_A,\boldsymbol x_B$ ，这两点连成的线段上的任意一点 $\boldsymbol x_C$ 都可以表示成

$$\boldsymbol x_C=\lambda\boldsymbol x_A+(1-\lambda)\boldsymbol x_B\tag 3$$
其中 $0<\lambda<1$ ，根据线性函数的基本性质，很容易得出 $\boldsymbol x_C$ 也属于 $\mathcal R_k$ 。
这种线性判别函数与最简单的线性回归形式基本类似，所以我们先考虑用最小平方训练模型，将所有偏置和参数向量聚集在一起，有 $y( {\boldsymbol X})={\boldsymbol  W}^T {\boldsymbol X}+\boldsymbol {w_0}$ ，其中 $\boldsymbol W^{D\times K}$ 代表 $K$ 个种类的参数， $\boldsymbol w_0$ 是所有偏置项， $\boldsymbol X^{D\times N}$ 是 $N$ 个训练集数据的矩阵，$\boldsymbol T^{K\times N}$ 是目标向量矩阵，平方和误差函数可以写成 $\boldsymbol w_0$

$$E({\boldsymbol X})=\frac{1}{2}\mathrm{Tr} \left \{ ({\boldsymbol W}^T {\boldsymbol X} +\boldsymbol {1w_0}-\boldsymbol T)^T ( {\boldsymbol W}^T{\boldsymbol X}  +\boldsymbol {1w_0}-\boldsymbol T) \right \}\tag 4$$
使用解析法可以计算出 ${\boldsymbol W}=\tilde {\boldsymbol X}^\dag \tilde {\boldsymbol T}$ ，其中 $\tilde {\boldsymbol X}^\dag$ 是 $\tilde {\boldsymbol X}$ 的伪逆矩阵， $\tilde {\boldsymbol X}=\boldsymbol X-\boldsymbol {1\bar{x}}^T$ ， $\boldsymbol {\bar x}^{D\times 1}$ 是训练数据的均值向量， $\tilde {\boldsymbol T}=\boldsymbol T-\boldsymbol {1\bar{t}}^T$ 。多⽬标变量的最⼩平⽅解有⼀个性质，如果训练集⾥的每个⽬标向量都满⾜某个线性限制

$$\boldsymbol a^T \boldsymbol t_n+b=0\tag 5$$
其中 $\boldsymbol a$ 和 $\boldsymbol b$ 为常数，那么对于任何 $\boldsymbol x$ 值，模型的预测也满⾜同样的限制，即

$$\boldsymbol a^T y(\boldsymbol x)+b=0\tag 6$$
将解析解直接代入，可得

$$y(\boldsymbol x^\star)=\boldsymbol W^T\boldsymbol x^\star+\boldsymbol w_0=\boldsymbol {\bar t}-\tilde{\boldsymbol T}^T(\boldsymbol {\tilde X}^\dag)^T(\boldsymbol x^\star-\boldsymbol {\bar x})\tag 7$$
其中 $\boldsymbol w_0$ 由公式 (6) 的一阶导置零求得。对公式 (8) 两边都乘以 $\boldsymbol a^T$ 并且根据 $\tilde {\boldsymbol T}=\boldsymbol T-\boldsymbol {1\bar{t}}^T$ 可得结果为 -b ，证明公式 (7) 成立。
最⼩平⽅法对于离群点缺乏鲁棒性，而且由于最⼩平⽅法对应于⾼斯条件分布假设下的最⼤似然法，多目标向量显然不是服从一个高斯分布，线性分类模型需要使用其他方法训练参数。