# [感知器算法 Perceptron Algorithm](./ch2_linear_classification/2.3_perceptron_algorithm.md)

> [2.1 线性判别分析 Linear Discriminate Analysis](./ch2_linear_classification/2.1_linear_discriminate_analysis.md) </br>
> [2.2 Fisher分类器 Fisher Classifier](./ch2_linear_classification/2.2_fisher_classifier.md) </br>
> [2.3 感知器算法 Perceptron Algorithm](./ch2_linear_classification/2.3_perceptron_algorithm.md) </br>
> [2.4 判别式 Logistic 回归 Discrminate Logistic Regression](./ch2_linear_classification/2.4_discriminate_logistic_regression.md) </br>
> [2.5 生成式 Logistic 回归 Generative Logistic Regression](./ch2_linear_classification/2.5_generative_logistic_regression.md) </br>
> [2.6 广义线性模型 Generalized Linear Model](./ch2_linear_classification/2.6_generalized_linear_model.md) </br>

线性判别模型的另⼀个例⼦是感知器算法 (perceptron algorithm)，它对应⼀个⼆分类模型，输⼊向量 $\boldsymbol x$ ⾸先使⽤⼀个固定的⾮线性变换得到⼀个特征向量 $\phi(\boldsymbol x)$ ，然后被⽤于构造⼀个线性模型，形式为

$$y(\boldsymbol x) =f(\boldsymbol w^T\phi(\boldsymbol x))\tag 1$$
其中非线性激活函数 $f(\cdot)$ 是一个阶梯函数，形式为

$$f(a)=\left\{\begin{matrix} +1,a\ge 0 \\ -1,a<0 \end{matrix}\right\}$$
向量 $\phi(\boldsymbol x)$ 通常包含⼀个偏置分量 $\phi_0(\boldsymbol x)=1$ 。我们仍可以使用误差最小化来确定感知器的参数 $\boldsymbol w$ ，可以将误分类的数据作为误差函数，但这样做会使误差函数变为 $\boldsymbol w$ 的分段常函数，无法正常使用梯度下降算法。考虑其他的误差函数，根据二分类的线性判别函数，我们可以寻找一个权向量 $\boldsymbol w$ 使得对 $C_1$ 类都有 $\boldsymbol w^T\phi(\boldsymbol x)>0$ ，对于 $C_2$ 类有 $\boldsymbol w^T\phi(\boldsymbol x)<0$ ，同时由于目标值正好异号，可以利用所有误分类的模式构造如下误差函数

$$E(\boldsymbol w)=-\sum_{n\in \mathcal M}\boldsymbol w^T\phi_nt_n\tag 3$$
对这个误差函数使⽤随机梯度下降算法，权向量 $\boldsymbol w$ 的变化为

$$\boldsymbol w^{(\tau+1)}=\boldsymbol w^{(\tau)}-\eta\nabla E(\boldsymbol w)=\boldsymbol w^{(\tau)}+\eta\phi_n t_n\tag 4$$
其中 $\eta$ 是学习率参数， $\tau$ 是迭代次数。如果我们将 $\boldsymbol w$ 乘以⼀个常数，那么感知器函数 $y=f(\boldsymbol x)$ 不变，因此我们可令学习率参数 $\eta$ 等于 1 ⽽不失⼀般性。我们对训练模式进⾏循环处理，对于每个数据点 $\boldsymbol x_n$ 计算感知器函数，如果模式正确分类，那么权向量保持不变，⽽如果模式被错误分类，那么对于类别 $C_1$ ，我们把向量 $\phi(\boldsymbol x_n)$ 加到当前权向量 $\boldsymbol w$ 的估计值上， ⽽对于类别 $C_2$ ，我们从 $\boldsymbol w$ 中减掉向量 $\phi(\boldsymbol x_n)$ 。
在线性判别函数一节中已经讲过，参数 $\boldsymbol w$ 实际上决定了决策界面的方向，感知器算法的迭代过程可以看做是将决策面朝着错误分类数据点的移动，如下图所示

<div align=center>
<img src="images/2_3_mlp1.png" width="80%"/>
</div>

如果我们考虑感知器学习算法中⼀次权值更新的效果，可以看到，⼀个误分类模式对于误差函数的贡献会逐渐减⼩，如下

$$-\boldsymbol w^{(\tau+1)T}\phi_nt_n=-\boldsymbol w^{(\tau)T}\phi_nt_n-(\phi_nt_n)^{T}\phi_nt_n<-\boldsymbol w^{(\tau)T}\phi_nt_n\tag 5$$
当然，这并不表明其他的误分类数据点对于误差函数的贡献会减⼩。此外，权向量的改变会使得某些之前正确分类的样本变为误分类。因此感知器学习规则并不保证在每个阶段都会减⼩整体的误差函数。如果数据点确实线性可分，那么感知器算法在经过一定迭代步骤一定可以找到精确解，但是所需要的步骤往往很大，并且在达到收敛状态之前，我们不能够区分不可分问题与缓慢收敛问题。即使数据集是线性可分的，也可能有多个解，并且最终收敛的解依赖于参数的初始化以及数据点出现的顺序。而对于线性不可分的数据集，感知器算法永远不会收敛。

本文讲述了三种线性分类的判别函数，下篇文章将从概率的角度观察分类问题。