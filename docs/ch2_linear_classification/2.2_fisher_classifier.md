# [Fisher分类器 Fisher Classifier](./ch2_linear_classification/2.2_fisher_classifier.md)

> [2.1 线性判别分析 Linear Discriminate Analysis](./ch2_linear_classification/2.1_linear_discriminate_analysis.md) </br>
> [2.2 Fisher分类器 Fisher Classifier](./ch2_linear_classification/2.2_fisher_classifier.md) </br>
> [2.3 感知器算法 Perceptron Algorithm](./ch2_linear_classification/2.3_perceptron_algorithm.md) </br>
> [2.4 判别式 Logistic 回归 Discrminate Logistic Regression](./ch2_linear_classification/2.4_discriminate_logistic_regression.md) </br>
> [2.5 生成式 Logistic 回归 Generative Logistic Regression](./ch2_linear_classification/2.5_generative_logistic_regression.md) </br>
> [2.6 广义线性模型 Generalized Linear Model](./ch2_linear_classification/2.6_generalized_linear_model.md) </br>

如果从维度降低的⾓度考察线性分类模型，对于二分类问题，可以看做针对输入向量 $\boldsymbol x$ 在一维空间的投影

$$y=\boldsymbol w^T\boldsymbol x\tag 1$$
线性判别函数等价于我们在 $y$ 上设置⼀个阈值，然后把 $y\geq-w_0$ 的样本分为 $C_1$ 类，把其余的样本分为 $C_2$ 类。 但是⼀维投影会造成相当多的信息丢失，因此在原始 $D$ 维空间能够完美分离的样本在⼀维空间中可能会相互重叠。Fisher 分类器提出的思想是最⼤化⼀个函数，能够让类均值的投影分开得较⼤，同时让每个类别内部的⽅差较⼩，从⽽最⼩化类别的重叠。仍然考虑上述二分类问题，包含 $N_1$ 个 $C_1$ 类的点和 $N_2$ 个 $C_2$ 类的点，两类的均值分别为 $\boldsymbol m_1=\frac{1}{N_1}\sum_{n\in C_1}\boldsymbol x_n$, $\boldsymbol m_2=\frac{1}{N_2}\sum_{n\in C_2}\boldsymbol x_n$ ，最简单的度量类间区分程度的⽅式就是类别均值投影之后的距离。取单位长度向量 $\boldsymbol w$ 并向其投影，取下式的最大值即可

$$m_2-m_1=\boldsymbol w^T(\boldsymbol m_2-\boldsymbol m_1)\tag 2$$
其中 $m_k=\boldsymbol w^T\boldsymbol m_k$ ，是类别 $C_k$ 的均值投影。但是如下图所示，当投影到一维空间时，就有了⼀定程度的重叠。如果类概率分布的协⽅差矩阵与对角化矩阵差距较⼤，即类内方差在各个方向上差异较大，那么这种问题就会出现。如下图左图所示

<div align=center>
<img src="images/2_2_fisher1.png" width="80%"/>
</div>

我们将投影在一维空间的类 $C_k$ 的类内方差记作

$$s_k^2=\sum_{n\in C_k}(y_n-m_k)^2\tag 3$$ 
我们把整个数据集的总的类内方差定义为 $s_1^2+s_2^2$ ，Fisher 准则根据类间⽅差和类内⽅差的⽐值定义，即

$$J(\boldsymbol w)=\frac{(m_2-m_1)^2}{s_1^2+s_2^2}=\frac{\boldsymbol w^T\boldsymbol S_B\boldsymbol w}{\boldsymbol w^T\boldsymbol S_W\boldsymbol w}\tag 4$$
分别用 $\boldsymbol S_B=(\boldsymbol m_2-\boldsymbol m_1)(\boldsymbol m_2-\boldsymbol m_1)^T$ 和 $\boldsymbol S_w=\sum_{n\in C_1}( \boldsymbol x_n-\boldsymbol m_1)(\boldsymbol x_n-\boldsymbol m_1)^T + \sum_{n\in C_2}( \boldsymbol x_n-\boldsymbol m_2)(\boldsymbol x_n-\boldsymbol m_2)^T$ 表示类间 (between-class) 协方差矩阵和类内 (within-class) 协方差矩阵。对公式 (12) 求导，发现 $J(\boldsymbol w)$ 取极大值的条件为

$$(\boldsymbol w^T\boldsymbol S_B\boldsymbol w)\boldsymbol S_W\boldsymbol w=(\boldsymbol w^T\boldsymbol S_W\boldsymbol w)\boldsymbol S_B\boldsymbol w\tag 4$$
考虑只是一维投影，因此我们只关心 $\boldsymbol w$ 的方向，忽略标量因子 $\boldsymbol w^T\boldsymbol S_B\boldsymbol w$ 和 $\boldsymbol w^T\boldsymbol S_W\boldsymbol w$ ，而 $\boldsymbol S_B\boldsymbol w$ 总是在 $\boldsymbol m_2-\boldsymbol m_1$ 的方向上，对公式 (13) 两侧同乘以 $\boldsymbol S_W^{-1}$ ，可得

$$\boldsymbol w\propto \boldsymbol S_W^{-1}(\boldsymbol m_2-\boldsymbol m_1)\tag 5$$
如上图右图就是最大化类间方差与类内方差比值的结果。如果类内协⽅差矩阵是各向同性的，即各个方向方差一致，协方差为正实数与单位矩阵相乘，从⽽ $\boldsymbol S_W$ 正⽐于单位矩阵，那么 $\boldsymbol w$ 正⽐于类均值的差。
最⼩平⽅⽅法确定线性判别函数的⽬标是使模型的预测尽可能地与⽬标值接近。相反，Fisher 判别准则的⽬标是使输出空间的类别有最⼤的区分度。这两种方法也并非毫无关系，我们可以通过修改目标向量建立二者的联系，对于⼆分类问题，Fisher 准则可以看成最⼩平⽅的⼀个特例。对于 $C_1$ 类，我们令其目标值为 $\frac{N}{N_1}$ ，而 $C_2$ 类为 $\frac{N}{N_2}$ ， $N_1,N_2$ 分别为类别 $C_1,C_2$ 数据点的个数，此时平方误差函数可以写成

$$E=\frac{1}{2}\sum_{n=1}^{N}(\boldsymbol w^T\boldsymbol x_n+w_0-t_n)^2\tag 6$$
令 $E$ 关于 $w_0$ 和 $\boldsymbol w$ 的导数为零，得

$$\sum_{n=1}^{N}(\boldsymbol w^T\boldsymbol x_n+w_0-t_n)=0\tag 7$$ 

$$\sum_{n=1}^{N}(\boldsymbol w^T\boldsymbol x_n+w_0-t_n)\boldsymbol x_n=0\tag 8$$
先求解公式 (16)，可得偏置表达式

$$w_0=-\boldsymbol w^T\boldsymbol m\tag 9$$ 
其中 $\sum_{n=1}^{N}t_n=N_1\frac{N}{N_1}-N_2\frac{N}{N_2}=0, \boldsymbol m=\frac {1}{N}(N_1\boldsymbol m_1+N_2\boldsymbol m_2)$ ，将这两个式子以及公式 (18) 代入 (17) 通过繁琐但并不复杂的运算，可得

$$(\boldsymbol S_W+\frac{N_1N_2}{N}\boldsymbol S_B)\boldsymbol w=N(\boldsymbol m_2-\boldsymbol m_1)\tag{10}$$
仍然可以得到与公式 (14) 类似的结果，因为最小二乘法的计算本就包含了类内方差的因子，我们只不过通过构造目标值引入类间方差从而得到 Fisher 分类器的结果。