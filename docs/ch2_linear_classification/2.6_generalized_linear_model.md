# [广义线性模型 Generalized Linear Model](./ch2_linear_classification/2.6_generalized_linear_model.md)

> [2.1 线性判别分析 Linear Discriminate Analysis](./ch2_linear_classification/2.1_linear_discriminate_analysis.md) </br>
> [2.2 Fisher分类器 Fisher Classifier](./ch2_linear_classification/2.2_fisher_classifier.md) </br>
> [2.3 感知器算法 Perceptron Algorithm](./ch2_linear_classification/2.3_perceptron_algorithm.md) </br>
> [2.4 判别式 Logistic 回归 Discrminate Logistic Regression](./ch2_linear_classification/2.4_discriminate_logistic_regression.md) </br>
> [2.5 生成式 Logistic 回归 Generative Logistic Regression](./ch2_linear_classification/2.5_generative_logistic_regression.md) </br>
> [2.6 广义线性模型 Generalized Linear Model](./ch2_linear_classification/2.6_generalized_linear_model.md) </br>

前面所介绍的线性模型实际上都是一个模型族的特例，这个模型族被称为广义线性模型 (generalized linear model) ，首先需要介绍相关的指数族分布。
## 指数族分布 (The Exponential Family)

前文中，我们使用最大似然法估计线性回归和线性分类模型参数时，都需要表示出目标值或向量的概率分布来建模，其中线性回归符合正态分布 $p(\boldsymbol  t|\boldsymbol x,\boldsymbol w)\sim \mathcal N(\boldsymbol \mu, \boldsymbol \sigma^2)$ ， logistic 回归对应伯努利分布 $p(\boldsymbol  t|\boldsymbol x,\boldsymbol w)\sim \mathrm{Bernoulli} (\rho)$ ，大部分概率分布（⾼斯混合分布除外）都是⼀⼤类被称为指数族 (exponential family) 概率分布的具体例⼦，它们有许多共同的性质。参数为 $\boldsymbol \eta$ 的变量 $\boldsymbol x$ 指数族分布可定义为具有下⾯形式的概率分布的集合（此处采用 Bishop PRML 中的写法，与 CS229 讲义中表示方法略有不同）

$$p(\boldsymbol x|\boldsymbol \eta)=h(\boldsymbol x)g(\boldsymbol \eta)\exp \left \{\boldsymbol \eta^T\phi(\boldsymbol x )\right \}\tag 1$$
其中 $\boldsymbol x$ 是输入向量（或标量亦可）， $\boldsymbol \eta$ 被称为概率分布的⾃然参数 (natural parameters)， $\boldsymbol \phi (\boldsymbol x)$ 是关于 $\boldsymbol x$ 的某个函数，被称为充分统计量 (sufficient statistic)，一般情况下我们仍取 $\boldsymbol\phi (\boldsymbol x)=\boldsymbol x$ 。函数 $g(\boldsymbol \eta)$ 可以被看成系数，它确保了概率分布的归⼀化，满⾜

$$g(\boldsymbol \eta)\int h(\boldsymbol x)\exp \left \{\boldsymbol \eta^T\boldsymbol\phi(\boldsymbol x )\right \}d\boldsymbol x=1\tag 2$$
下面我们来证明一下前文涉及的概率分布属于指数族分布。

### 伯努利分布

伯努利分布可以表示成

$$\begin{align} p(x|\mu)=\mu^x(1-\mu)^{1-x}&=\exp\left \{ x\ln \mu+(1-x)\ln(1-u) \right \}\\ &=(1-\mu)\exp\left \{ \ln(\frac{\mu}{1-\mu})x \right \} \end{align}$$
可以看出 $\eta=\ln(\frac{\mu}{1-\mu})$ ，反之 $\mu=\sigma(\eta)=\frac{1}{1+\exp(-\eta)}$ ，正好是 logistic sigmoid 函数。使用公式 (28) 的标准型就可将伯努利分布写成下面的形式

$$p(x|\mu)=\sigma(-\eta)\exp(\eta x)\tag 3$$
其中利用了 logistic sigmoid 的对称性 $\sigma(-\eta)=1-\sigma(\eta)$ ，并有 $\mu(x)=x ， h(x)=1 ， g(\eta)=\sigma(-\eta)$ 。

### 多项分布

接下来考虑单⼀观测 $\boldsymbol x$ 的多项式分布，形式为

$$p(\boldsymbol x|\boldsymbol \mu)=\exp\left \{ \prod_{k=1}^{M}x_k\ln \mu_k \right \}\tag 4$$
令 $\eta_k=\ln \mu_k$ ，我们有

$$p(\boldsymbol x|\boldsymbol \mu)=\exp(\boldsymbol \mu^T\boldsymbol x)\tag 5$$
需要注意参数 $\eta_k$ 不是相互独⽴的，因为要满⾜以下限制

$$\sum_{k=1}^{M}\mu_k=1\tag 6$$
因此给定任意 $M-1$ 个参数 $\mu_k$ ，剩下参数就固定了。一般情况下去掉这个限制⽐较⽅便。如果考虑此限制，我们就只⽤ $M-1$ 个参数来表示这个分布。使⽤公式 (34) 的关系，把 $\mu_M$ ⽤剩余的 $\left \{\mu_k \right \}$ 表示，其中 $k=1,2,...,M-1$ ，这样就只剩下了 $M-1$ 个参数。剩余的参数仍满⾜以下限制

$$0\leq \mu_k\leq1,\sum_{k=1}^{M-1}\mu_k\leq1\tag 6$$
关于公式 (32)，我们进一步可以写成

$$\exp \left \{ \sum_{k=1}^{M-1}x_k\ln \mu_k+(1- \sum_{k=1}^{M-1}x_k) \ln(1-\sum_{k=1}^{M-1}x_k)\right \}=\exp \left \{ \sum_{k=1}^{M-1}x_k\ln (\frac{\mu_k}{1-\sum_{k=1}^{M-1}\mu_k})+\ln(1-\sum_{k=1}^{M-1}\mu_k)\right \}\tag 7$$
整理可得 $\mu_k=\frac{\exp(\eta_k)}{1+ {\textstyle \sum_{j}^{}}\exp(\eta_j) }$ ，也就是常见的 Softmax 函数。在这种表达形式下，多项式分布为

$$p(\boldsymbol x|\boldsymbol \mu)=(1+\sum_{k=1}^{M-1}\exp(\mu_k))^{-1}\exp(\boldsymbol \mu^T\boldsymbol x)\tag 8$$

### 高斯分布

对于⼀元⾼斯分布，我们有

$$\begin{align} p(x|\mu,\sigma^2)&=\frac{1}{(2\pi\sigma^2)^{\frac{1}{2}}}\exp\left \{ -\frac{1}{2\sigma^2}(x-\mu)^2 \right \}\\ &=\frac{1}{(2\pi\sigma^2)^{\frac{1}{2}}}\exp\left \{ -\frac{1}{2\sigma^2}x^2+\frac{\mu}{\sigma^2}x-\frac{1}{2\sigma^2}\mu^2 \right \} \end{align} $$
令 $\boldsymbol \eta=[\frac{\mu}{\sigma^2},-\frac{1}{2\sigma^2}] ， \boldsymbol \phi(x)=[x,x^2] ， h(x)=(2\pi)^{-\frac{1}{2}} ， g(\boldsymbol \eta)=(-2\eta_2)^{\frac{1}{2}}\exp(\frac{\eta_1^2}{4\eta_2})$ ，高斯分布就可以转化为标准指数族分布的形式。

### 最⼤似然与充分统计量

考虑⽤最⼤似然法估计公式 (28) 给出的⼀般形式的指数族分布的参数向量 $\boldsymbol\phi(\boldsymbol x)$ 的问题。对公式 (29) 两侧求梯度，得

$$\nabla g(\boldsymbol \eta)\int h(\boldsymbol x)\exp \left \{\boldsymbol \eta^T\boldsymbol\phi(\boldsymbol x )\right \}d\boldsymbol x+g(\boldsymbol \eta)\int h(\boldsymbol x)\exp \left \{\boldsymbol \eta^T\boldsymbol\phi(\boldsymbol x )\right \}\boldsymbol\phi(\boldsymbol x )d\boldsymbol x=0\tag 9$$
重新排列并再次使用公式 (29)，可得

$$-\frac{1}{g(\boldsymbol \eta)}\nabla g(\boldsymbol \eta)=g(\boldsymbol \eta)\int h(\boldsymbol x)\exp \left \{\boldsymbol \eta^T\boldsymbol\phi(\boldsymbol x )\right \}\boldsymbol\phi(\boldsymbol x )d\boldsymbol x=\mathbb{E}[\boldsymbol\phi(\boldsymbol x)]\tag{10}$$

$$-\nabla \ln g(\boldsymbol \eta)=\mathbb{E}[\boldsymbol\phi(\boldsymbol x)]\tag{11}$$
$\boldsymbol\phi (\boldsymbol x)$ 的协⽅差可以根据 $g(\boldsymbol \eta)$ 的⼆阶导表达，对于⾼阶矩的情形也类似。因此，如果我们能对⼀个来⾃指数族分布的概率分布进⾏归⼀化，那么我们总能通过简单的求微分的⽅式找到它的矩。

现在考虑⼀组独⽴同分布的数据 $\boldsymbol X=\left \{ \boldsymbol x_1,...,\boldsymbol x_n\right \}$ 。对于这个数据集，似然函数为

$$p(\boldsymbol X\mid \boldsymbol \eta)=(\prod_{n=1}^{N}h(\boldsymbol x_n))g(\boldsymbol \eta)^N\exp \left \{ \boldsymbol \eta^T\sum_{n=1}^{N}\boldsymbol\phi (\boldsymbol x_n) \right \}\tag{12}$$
取负对数似然函数并令关于 $\boldsymbol \eta$ 的导数等于零，我们可以得到最⼤似然估计 $\boldsymbol\phi_{ML}$ 满⾜的条件

$$-\nabla \ln g(\boldsymbol \eta_{ML})=\frac{1}{N}\sum_{n=1}^{N}\boldsymbol\phi(\boldsymbol x_n)\tag{13}$$
原则上可以通过解这个⽅程来得到 $\boldsymbol\eta_{ML}$ 。我们看到最⼤似然估计的解只通过 $\sum_{n=1}^{N}\boldsymbol\phi(\boldsymbol x_n)$  对数据产⽣依赖，因此这个量被称为的充分统计量，所以最大似然解只依赖于充分统计量。对伯努利分布，函数 $\boldsymbol\phi (\boldsymbol x)=\boldsymbol x$ ，因此我们只需要关注数据点 $\left \{ x_n \right \}$ 的和即可。⽽对于⾼斯分布，$\boldsymbol \phi(x)=[x,x^2]$ ，因此我们应同时关注 $\left \{ \boldsymbol x_n \right \}$ 的和以及 $\left \{ \boldsymbol x_n^2 \right \}$ 的和。
如果考虑极限 $N\rightarrow+\infty$ ，那么公式 (43) 的右侧变成了 $[\boldsymbol\phi(\boldsymbol x_n)]$ ， 通过与公式 (13) ⽐较，可以看到在这个极限的情况下， $\boldsymbol\eta_{ML}$ 与真实值 $\boldsymbol\eta$ 相等。

## 构建广义线性模型 (Constructing Generalized Linear Model)

正如伯努利分布，多项分布，高斯分布与指数族分布的关系一样，线性回归，逻辑回归等也是一个更为广泛的广义线性模型的例子。为方便和以前公式对照，指数族分布对应的应该是目标值 $\boldsymbol y$ ，所以下面讨论需要将 $\boldsymbol x$ 替换成 $\boldsymbol y$ ，另外将 $\boldsymbol x$ 定义为线性模型的输入，并令 $\boldsymbol y=f(\boldsymbol x)$ 。
广义线性模型的构建需要基于以下三条假设：

1. $p(\boldsymbol y|\boldsymbol \eta,\boldsymbol x)$ 符合以 $\eta$ 为参数的指数族分布。
2. 给定输入 $\boldsymbol x$ ，我们的目标是预测 $\phi(\boldsymbol y)$ （注意这里将第一节的 $x$ 全部用目标值 $y$ 代替）的理想值，一般取 $\phi(\boldsymbol y) =\boldsymbol y$ ，正好是目标值。此时 $f$ 应满足 $h(\boldsymbol x)=E[\boldsymbol y\mid \boldsymbol x]$ 。
3. 自然参数 $\boldsymbol\eta$ 和输入 $\boldsymbol x$ 满足线性关系 $\boldsymbol \eta=\boldsymbol w^T \boldsymbol x$ 

在指数族分布的推导过程中，我们已经发现，如果认为上述三条假设成立，我们已经得出了相应的 logistic sigmoid 函数，softmax 函数，以及更一般的线性函数，分别对应二分类，多分类，以及线性回归的基础模型，所以他们都属于广义线性模型。
至此，线性分类也告一段落，虽然现在主流 deep learning 的任务都是以分类为主，但是成熟的模型体系从训练到应用似乎都不用了解这些基础的分类算法，然而那些高级算法总归还是脱胎于这些基础的理论，扎实的基本功对于提高学科上限还是很重要的。下篇文章将介绍神经网络。