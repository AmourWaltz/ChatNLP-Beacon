# [生成式 Logistic 回归 Generative Logistic Regression](./ch2_linear_classification/2.5_generative_logistic_regression.md)

> [2.1 线性判别分析 Linear Discriminate Analysis](./ch2_linear_classification/2.1_linear_discriminate_analysis.md) </br>
> [2.2 Fisher分类器 Fisher Classifier](./ch2_linear_classification/2.2_fisher_classifier.md) </br>
> [2.3 感知器算法 Perceptron Algorithm](./ch2_linear_classification/2.3_perceptron_algorithm.md) </br>
> [2.4 判别式 Logistic 回归 Discrminate Logistic Regression](./ch2_linear_classification/2.4_discriminate_logistic_regression.md) </br>
> [2.5 生成式 Logistic 回归 Generative Logistic Regression](./ch2_linear_classification/2.5_generative_logistic_regression.md) </br>
> [2.6 广义线性模型 Generalized Linear Model](./ch2_linear_classification/2.6_generalized_linear_model.md) </br>

现在讨论分类问题的概率生成式模型，生成式模型对类条件概率密度 $p(\boldsymbol x|C_k)$ 和类先验概率 $p(C_k)$ 建模，然后通过贝叶斯定理计算后验概率 $p(C_k|\boldsymbol x)$ 。不难看出，生成模型只需进行统计技术来获得模型，且对噪声有更好的鲁棒性；但由于需要人为设定先验分布，因此生成模型准确率并不高。
同样先只考虑二分类情形，我们直接应用贝叶斯定理对类别 C_1 的后验概率建模

$$p(C_1|\boldsymbol x)=\frac{p(\boldsymbol x|C_1)p(C_1)}{p(\boldsymbol x|C_1)p(C_1)+p(\boldsymbol x|C_2)p(C_2)}=\frac{1}{1+\exp{(-a)}}=\sigma(a)\tag 1$$
其中定义了 $a=\ln\frac{p(\boldsymbol x|C_1)p(C_1)}{p(\boldsymbol x|C_2)p(C_2)}$ 。虽然我们只是把后验概率写成了⼀个等价的形式，引入logistic sigmoid 函数似乎没有意义，然⽽，假设 $a(\boldsymbol x)$ 的函数形式相当简单，如上一节判别式模型所讲，考虑 $a(\boldsymbol x)$ 是 $\boldsymbol x$ 的线性函数的情形，这种情况下，后验概率由⼀个通⽤的线性模型确定，下文会予以证明。对于 $K>2$ 的情形，有

$$p(C_k|\boldsymbol x)=\frac{p(\boldsymbol x|C_k)p(C_k)}{ {\textstyle \sum_{j}^{}} p(\boldsymbol x|C_j)p(C_j)}=\frac{\exp(a_k)}{ {\textstyle \sum_{j}^{}} \exp(a_j)}\tag 2$$
其中 $a_k=\ln p(\boldsymbol x|C_k)p(C_k)$ ，上式也被称为归⼀化指数 (normalized exponential)，可以被当做 logistic sigmoid 函数对于多类情况的推⼴，也就是我们很熟悉的 Softmax 函数，因为它表示 “max” 函数的⼀个平滑版本，因为如果对所有的 $j\ne k$ 都有 $a_k\gg a_j$ ，那么 $p(C_k|\boldsymbol x)\simeq 1$ 且 $p(C_j|\boldsymbol x)\simeq 0$ 。

假设类条件概率密度是⾼斯分布，然后求解后验概率。这种生成式算法在 CS229 中也称作高斯判别模型 (Gaussian discriminative model)。⾸先，我们假定所有类别的协⽅差矩阵相同，这样类别 $C_k$ 的类条件概率为

$$p(\boldsymbol x|C_k)=\frac{1}{(2\pi)^{\frac{D}{2}}}\frac{1}{|\boldsymbol\Sigma|^{\frac{1}{2}}}\exp\left \{ -\frac{1}{2}(\boldsymbol x-\boldsymbol \mu_k)^T \boldsymbol\Sigma^{-1} (\boldsymbol x-\boldsymbol \mu_k) \right \}\tag 3$$
考虑二分类情形，我们直接将上述类条件概率密度代入 $a=\ln\frac{p(\boldsymbol x|C_1)p(C_1)}{p(\boldsymbol x|C_2)p(C_2)}$ ，假设类概率的协⽅差矩阵相同，可消除⾼斯概率密度的指数项中 $\boldsymbol x$ 的⼆次型。最后可以将公式化简为 $p(C_1|\boldsymbol x)=\sigma(\boldsymbol w^T\boldsymbol x+w_0)$ ，其中

$$\boldsymbol w=\boldsymbol \Sigma^{-1}(\boldsymbol \mu_1-\boldsymbol \mu_2)\tag 4$$

$$w_0=-\frac{1}{2}\boldsymbol \mu_1^T\boldsymbol \Sigma^{-1}\boldsymbol \mu_1+\frac{1}{2}\boldsymbol \mu_2^T\boldsymbol \Sigma^{-1}\boldsymbol \mu_2+\ln\frac{p(C_1)}{p(C_2)}\tag 5$$
从⽽得到了参数为 $\boldsymbol x$ 的线性函数的 logistic sigmoid 函数，最终求得的决策边界对应于后验概率 $p(C_k|\boldsymbol x)$ 为常数的决策⾯，因此由 $\boldsymbol x$ 的线性函数给出，从⽽决策边界在输⼊空间是线性的。先验概率密度只出现在偏置参数 $w_0$ 中，因此先验的改变的效果是平移决策边界，即平移后验概率中的常数轮廓线。

下图给出了⼆维输⼊空间 $\boldsymbol x$ 的结果，左图给出了两个类别的类条件概率密度，分别⽤红⾊和蓝⾊表示。右图给出了对应后验概率分布 $p(C_1|\boldsymbol x)$ ，它由 $x$ 的线性函数的 logistic sigmoid 函数给出。右图的曲⾯的颜⾊中，红⾊所占的⽐例由 $p(C_1|\boldsymbol x)$ 给出，蓝⾊所占的⽐例由 $p(C_2|\boldsymbol x)=1-p(C_1|\boldsymbol x)$ 给出。

<div align=center>
<img src="images/2_5_lg1.png" width="80%"/>
</div>

## 最大似然参数估计

⼀旦我们具体化了类条件概率密度 $p(\boldsymbol x|C_k)$ 的参数化函数形式，我们就能够使⽤最⼤似然法确定参数的值，以及先验类概率 $p(C_k)$ 。这需要数据集由观测 $\boldsymbol x$ 以及对应的类别标签组成。
依然只考虑二分类的情形，每个类别都有⼀个⾼斯类条件概率密度，且协⽅差矩阵相同。给定包含 $N$ 个数据点的数据集 $\mathcal D=\left \{ \boldsymbol x_n,t_n \right \}$ 。取类别标签 $t\in\left \{1,0 \right \}$ 分别对应 $C_1,C_2$。我们把先验概率记作 $p(C_1)=\pi$ ，从而 $p(C_2)=1-\pi$ 。对于⼀个来⾃类别 $C_1$ 的数据点 $\boldsymbol x_n$ ，有

$$p(\boldsymbol x_n,C_1)=p(C_1)p(\boldsymbol x_n|C_1)=\pi\mathcal N(\boldsymbol x_n|\boldsymbol \mu_1, \boldsymbol \Sigma)\tag 6$$
于是似然函数为

$$p(\boldsymbol t,\boldsymbol X|\pi,\boldsymbol \mu_1,\boldsymbol \mu_2,\boldsymbol \Sigma )=\prod_{n=1}^{N} [ \pi\mathcal N(\boldsymbol x_n|\boldsymbol \mu_1, \boldsymbol \Sigma)]^{t_n}[(1-\pi)\mathcal N(\boldsymbol x_n|\boldsymbol \mu_2, \boldsymbol \Sigma)]^{1-t_n}\tag 7$$
仍然最大化负对数似然函数，依次对四个参数 $\pi,\boldsymbol \mu_1,\boldsymbol \mu_2,\boldsymbol \Sigma$ 求偏导，先考虑关于 $\pi$ 的最大化，对数似然函数与 $\pi$ 相关的项为 $\sum_{n=1}^{N}\left \{ t_n\ln \pi+(1-t_n)\ln (1-\pi) \right \}$ ，令其关于 $\pi$ 的导数等于零，整理可得

$$\pi=\frac{1}{N}\sum_{n=1}^{N}t_n=\frac{N_1}{N_1+N_2}\tag 8$$
其中 $N_1,N_2$ 分别表示类别 $C_1,C_2$ 的数据点总数，因此， $\pi$ 的最⼤似然估计就是类别 $C_1$ 的点所占的⽐例，这与我们预期的相同。

接着考虑关于 $\boldsymbol \mu_1$ 的最⼤化。与之前⼀样，我们把对数似然函数中与 $\boldsymbol \mu_1$ 相关的量挑出来，即 $-\frac{1}{2}\sum_{n=1}^{N}t_n(\boldsymbol x_n-\boldsymbol \mu_1)^T\boldsymbol \Sigma^{-1}(\boldsymbol x_n-\boldsymbol \mu_1)$ ，令其关于 $\boldsymbol \mu_1$ 的导数等于零，整理可得

$$\boldsymbol \mu_1 =\frac{1}{N_1}\sum_{n=1}^{N}t_n\boldsymbol x_n\tag 9$$
这就是属于类别 $C_1$ 的输⼊向量 $\boldsymbol x_n$ 的均值。通过类似的推导，对应 $\boldsymbol \mu_2$ 的结果为

$$\boldsymbol \mu_2 =\frac{1}{N_2}\sum_{n=1}^{N}(1-t_n)\boldsymbol x_n\tag{10}$$
最后，考虑协⽅差矩阵 $\boldsymbol \Sigma$ 的最⼤似然解，选出与 $\boldsymbol \Sigma$ 相关的项 $-\frac{N}{2}\ln |\boldsymbol \Sigma|-\frac{N}{2}\mathrm{Tr}\left \{ \boldsymbol \Sigma^{-1}\boldsymbol S \right \}$ ，使⽤⾼斯分布的最⼤似然解的标准结果可得 $\boldsymbol \Sigma=\boldsymbol S$ ，其中

$$\boldsymbol S=\frac{N_1}{N}·\frac{1}{N_1}\sum_{n\in C_1}(\boldsymbol x-\boldsymbol \mu_1)(\boldsymbol x-\boldsymbol \mu_1)^T+\frac{N_2}{N}·\frac{1}{N_2}\sum_{n\in C_2}(\boldsymbol x-\boldsymbol \mu_2)(\boldsymbol x-\boldsymbol \mu_2)^T\tag{11}$$
它表示对⼀个与两类都有关系的协⽅差矩阵求加权平均。拟合类⾼斯分布的⽅法对于离群点并不鲁棒，因为⾼斯的最⼤似然估计是不鲁棒的。

## 生成式模型与判别式模型

对于一般的线性判别模型，如果输入向量 $\boldsymbol x$ 具有 $M$ 维特征，那么这个模型有 $M$ 个可调节参数。对于上述二分类的生成式模型，如果我们使⽤最⼤似然⽅法调节⾼斯类条件概率密度，那么我们有 $2M$ 个参数来描述均值， 以及 $\frac{M(M+1)}{2}$ 个参数来描述协⽅差矩阵，仍然假设协⽅差矩阵相同，算上类先验，参数总数量为 $\frac{M(M+5)}{2}+1$ ，这随着 $M$ 的增长以⼆次的⽅式增长。这和判别式模型对于参数数量 $M$ 的线性依赖不同，对于大的 $M$ 值，生成式模型参数量通常很大。可以看出，生成式模型相比判别式模型，建模更加复杂，使用也比较受限。

通过公式 (17) 我们已经发现，逻辑回归的判别式模型和生成式模型有着相同的形式，但对于相同的数据集两种算法会给出不同的边界，有一个结论是：如果 $p(\boldsymbol x| C_k)$ 属于多元高斯分布且共享协方差矩阵 $\Sigma$ ，那么 $p(C_k|\boldsymbol x)$ 一定是逻辑函数，反之不成立。在这些假设都正确的前提下，生成式模型效果往往更好，特别是对于较少的训练集，也可以取得更好的效果。相反，判别式模型由于假设相对简单，主要靠大量数据集训练，因此对于错误的模型假设不那么敏感。
