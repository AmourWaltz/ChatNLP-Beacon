# [判别式 Logistic 回归 Discrminate Logistic Regression](./ch2_linear_classification/2.4_discriminate_logistic_regression.md)

> [2.1 线性判别分析 Linear Discriminate Analysis](./ch2_linear_classification/2.1_linear_discriminate_analysis.md) </br>
> [2.2 Fisher分类器 Fisher Classifier](./ch2_linear_classification/2.2_fisher_classifier.md) </br>
> [2.3 感知器算法 Perceptron Algorithm](./ch2_linear_classification/2.3_perceptron_algorithm.md) </br>
> [2.4 判别式 Logistic 回归 Discrminate Logistic Regression](./ch2_linear_classification/2.4_discriminate_logistic_regression.md) </br>
> [2.5 生成式 Logistic 回归 Generative Logistic Regression](./ch2_linear_classification/2.5_generative_logistic_regression.md) </br>
> [2.6 广义线性模型 Generalized Linear Model](./ch2_linear_classification/2.6_generalized_linear_model.md) </br>

上一篇文章讨论了几种线性判别函数，这些判别函数都是将分类的推断及决策合而为一的，如果分成两个阶段讨论，针对决策，很自然联想使用概率表示分类的可能性，所以我们可以将分类结果映射到区间 $[0,1]$ 上再决策。在原始输入空间中有很多已知类型的数据点，我们需要建立合适的模型对这些点进行区分。最简单的情况就是这些数据点是线性可分的，利用上一章的线性判别函数就能区分，但是在一些复杂空间，类条件概率密度 $p(\boldsymbol x|C_k)$ 有相当大的重叠，也就是对于某些数据点不能简单粗暴的认为属于或不属于某一类，我们需要一种更加细致定量的概率方法来表示，所以才引入对后验概率的建模。在此引入逻辑回归 (logistic regression) ，将输出映射至 $[0,1]$ 的概率空间上，logistic 函数一般形式如下，后续讨论会发现 logistic 回归一些特性很有助于我们分析分类问题。

$$\sigma(a)=\frac{1}{1+\exp(-a)}\tag 1$$

我们平常所熟知的利用 logistic 回归做分类一般都是判别式模型。上文已经讨论过，判别式模型就是根据直接对后验概率 $p(C_k|\boldsymbol x)$ 精确建模，这种方法很直观，准确率高，可直接学习求得参数，简化学习。

在线性回归一节中提到，对输入向量做非线性变换可以提取出某些高级特征，所以我们可以引入一些非线性函数做分类。⾮线性变换不会消除数据类型重叠，甚至会增加重叠的程度，但恰当地选择⾮线性变换能够让后验概率的建模过程更简单。在此选择 logistic sigmoid 函数，下图给出了这个函数的图像。“sigmoid” 的意思是 “S形”。这种函数有时被称为“挤压函数”，因为它把整个实数轴映射到了⼀个有限的区间中

<div align=center>
<img src="images/2_4_lg1.png" width="80%"/>
</div>

它满⾜下⾯的对称性

$$\sigma(-a)=1-\sigma(a)\tag 2$$

## 最大似然参数估计

我们现在使⽤最⼤似然⽅法来确定 logistic 回归模型的参数。为了完成这⼀点，我们要使⽤ logistic sigmoid 函数的导数，它可以很⽅便地使⽤sigmoid函数本⾝表示如下：

$$\frac{d\sigma}{da}=\sigma(1-\sigma)\tag 3$$
先考虑最简单的二分类情形，我们定义输入 $\boldsymbol x$ 在类 $C_1$ 上的后验概率为

$$p(C_1|\boldsymbol x)=y(\boldsymbol x, \boldsymbol w)=\sigma (\boldsymbol w^T\boldsymbol x)\tag 4$$
那么 $p(C_2|\boldsymbol x)=1-p(C_1|\boldsymbol x)$ ，此时整体分布恰好是一个伯努利分布 (Bernoulli distribution)。将上式合并，取类别标签 $t\in\left \{1,0 \right \}$ 分别对应 $C_1,C_2$ ，那么 $p(C_{k\in\left \{ 1,2\right \}}|\boldsymbol x)=y^{t}(1-y)^{1-t}$ 。更一般地，对于⼀个数据集 $\mathcal{D}$ ，包含 $N$ 个相互独立的输入数据 $\boldsymbol x$ 和标签 $\boldsymbol t=\left \{ t_1, t_2,...,t_N \right \}$ ，其中 $t_n\in\left \{0,1 \right \} ， y_n=p(C_1|\boldsymbol x_n, \boldsymbol w)=\sigma(\boldsymbol w^T\boldsymbol x_n)$ ，此时似然函数可以写成

$$p(\boldsymbol t|\boldsymbol w)=\prod_{n=1}^{N}y_n^{t_n}(1-y_n^{1-t_n})\tag 5$$
与之前⼀样，我们可以通过取似然函数负对数的⽅式，定义⼀个误差函数，由此引入交叉熵 (cross-entropy) 误差函数，形式为

$$E(\boldsymbol w)=-\ln{p(\boldsymbol t|\boldsymbol w)}=-\sum_{n=1}^{N}\left \{ t_n\ln y_n +(1-t_n)\ln{(1-y_n)} \right \}\tag 6$$
两侧取误差函数关于 $\boldsymbol w$ 的梯度，结合公式 (3) 将涉及到 logistic sigmoid 的导数的因⼦消去，我们有

$$\nabla E(\boldsymbol w)=\sum_{n=1}^{N}(y_n-t_n)\boldsymbol x_n\tag 7$$
可见数据点 $n$ 对梯度的贡献为⽬标值和模型预测值之间的误差 $y_n-t_n$ 与输入向量 $\boldsymbol x_n$ 相乘，这种函数形式与线性回归模型中的平⽅和误差函数的梯度形式完全相同。 

最⼤似然法对于线性可分的数据集会产⽣严重的过拟合现象，这是由于最⼤似然解出现在超平⾯对应于 \sigma=0.5 一阶导数最大的情况，由于线性可分的数据集具有如下性质：

$$\boldsymbol w^T\boldsymbol x_n=\left\{\begin{matrix} \ge 0,~~~ if~~t_n=1 \\ <1, ~~~ otherwise \end{matrix}\right\}$$
等价于 $\boldsymbol w^T\boldsymbol x=\boldsymbol 0$ ，最⼤似然解把数据集分成两类，一阶导为零等价于 $y_n=\sigma(\boldsymbol w^T\phi_n)=t_n$ ， $\boldsymbol w$ 趋向于⽆穷⼤。这种情况下，logistic sigmoid 函数在特征空间中变得⾮常陡峭，对应于⼀个跳变的阶梯函数，使得每⼀个来⾃类别 $k$ 的训练数据都被赋予⼀个后验概率 $p(C_k|\boldsymbol x)=1$ 。最⼤似然法⽆法区分某个解优于另⼀个解，并且在实际应⽤中哪个解被找到将会依赖于优化算法的选择和参数的初始化（大多数模型都这样）。即使训练数据远大于参数数量，只要数据线性可分，这个问题就会出现。可以通过引⼊类先验概率，然后寻找 $\boldsymbol w$ 的最大后验解，或者给误差函数增加⼀个正则化项， 这种奇异性就可以被避免。

## 牛顿法-迭代重加权最⼩平⽅

在线性回归第二篇文章中曾讨论了利用误差函数二阶导更新梯度的牛顿法 (Newton-Raphson)。线性回归的最小平方误差函数是关于参数 $\boldsymbol w$ 的二次凸函数，最大似然解具有解析解，对应于在牛顿法中误差函数是一个正定二次函数，应用一次就能跳到最小值点；logistic 回归模型的误差函数中不具备解析解，但它是一个凸函数仍可以局部近似成正定二次函数，因此也存在唯一最小值。牛顿法对权值的更新形式为

$$\boldsymbol w^{\mathrm{new}}=\boldsymbol w^{\mathrm{old}}-\boldsymbol H^{-1}\nabla E(\boldsymbol w)\tag 9$$ 
其中 $\boldsymbol H$ 是一个 Hessian 矩阵，它的元素由 $E(\boldsymbol w)$ 关于 $\boldsymbol w$ 的二阶导构成。将牛顿法应用到交叉熵误差函数上，误差函数的梯度和 Hessian 矩阵为

$$\nabla E(\boldsymbol w)=\sum_{n=1}^{N}(y_n-t_n)\boldsymbol x_n=\boldsymbol X^T(\boldsymbol y-\boldsymbol t)\tag{10}$$

$$\boldsymbol H=\nabla\nabla E(\boldsymbol w)=\sum_{n=1}^{N}y_n(1-y_n)\boldsymbol x_n\boldsymbol x_n^T=\boldsymbol X^T\boldsymbol R\boldsymbol X\tag{11}$$
$y_n(1-y_n)$ 由公式 (3) 所得，仔细观察会发现，如果把误差函数换成最小平方误差，Hessian 矩阵就可以消掉 $\boldsymbol R$ ，代入公式 (9)，就可以得出最小平方误差的解析解形式，对应于第三篇文章解析法的基函数取 $\phi(\boldsymbol x)=\boldsymbol x$ ，所以解析法和牛顿法在二次凸函数上的应用原理是相同的。再回到交叉熵函数，我们引入一个 $N\times N$ 的对角矩阵 $\boldsymbol R$ ，元素为 $R_{nn}=y_n(1-y_n)$ ，看到 Hessian 矩阵不再是常量，⽽是通过权矩阵 $\boldsymbol R$ 仍含有 $\boldsymbol w$ ，对应于误差函数不是⼆次函数的事实。

由于 $0<y_n<1$ ，因此对于任意向量 $\boldsymbol u$ 都有 $\boldsymbol u^T\boldsymbol H\boldsymbol u>0$ ，因此 Hessian 矩阵 $\boldsymbol H$ 是正定的，误差函数是 $\boldsymbol w$ 的⼀个凸函数，从⽽有唯⼀的最⼩值。这样 logistic 回归模型更新公式就如下

$$\begin{align} \boldsymbol w^{\mathrm {new}}&=\boldsymbol w^{\mathrm {old}}-(\boldsymbol X^T\boldsymbol R\boldsymbol X)^{-1}\boldsymbol X^T(\boldsymbol y-\boldsymbol t)\\ &=(\boldsymbol X^T\boldsymbol R\boldsymbol X)^{-1}\left \{ \boldsymbol X^T\boldsymbol R\boldsymbol X\boldsymbol w^{\mathrm {old}}-\boldsymbol X^T(\boldsymbol y-\boldsymbol t) \right \} \\ &=(\boldsymbol X^T\boldsymbol R\boldsymbol X)^{-1}\boldsymbol X^T\boldsymbol R\boldsymbol z \end{align}$$
由于权矩阵 $\boldsymbol R$ 不是常量，⽽是依赖于参数向量 $\boldsymbol w$ ， 因此我们必须迭代地应⽤牛顿法的解，每次使⽤新的权向量 $\boldsymbol w$ 计算⼀个修正的权矩阵 $\boldsymbol R$ ，这个算法也被称为迭代重加权最⼩平⽅ (iterative reweighted least squares)。logistic 回归模型 t 的均值和⽅差分别为

$$\mathbb{E}\left [ t \right ] =\sigma(\boldsymbol x)=y\tag{13}$$

$$\mathrm{var}\left [ t \right ] =\mathbb{E}\left [ t^2 \right ] -\mathbb{E}\left [ t \right ] ^2=\sigma(\boldsymbol x)-\sigma(\boldsymbol x)^2=y(1-y)\tag{14}$$
$t\in\left \{ 0,1 \right \}$ 时， $t^2=t$ ，所以对角矩阵 $\boldsymbol R$ 也可以看成⽅差。我们可以把迭代重加权最⼩平⽅看成变量空间 $a_n=\boldsymbol w^T\boldsymbol x_n$ 的线性问题的解。这样， $\boldsymbol z$ 的第 $n$ 个元素 $\boldsymbol z_n$ 就可以简单看成这个空间中的有效⽬标值。这里引入 logistic sigmoid 的反函数

$$a=\ln{\frac{\sigma}{1-\sigma}}\tag{15}$$
$\boldsymbol z_n$ 可以通过对当前操作点 $\boldsymbol w^{\mathrm{old}}$ 附近的 logistic sigmoid 函数的局部线性近似的⽅式得到。

$$a_n({\boldsymbol w})\simeq a_n({\boldsymbol w^{\mathrm{old}}})+\frac{\mathrm{d} a_n}{\mathrm{d} y_n}|_{\boldsymbol w^{\mathrm{old}}}(t_n-y_n)=\boldsymbol x^T\boldsymbol w^{\mathrm{old}}-\frac{y_n-t_n}{y_n(1-y_n)}=z_n\tag{16}$$
