# 知识对话和人设对话 Knowledge-Grounded and Persona Dialogues

## 基于知识的对话大模型

&emsp;&emsp;ChatGPT 的横空出世，在整个自然语言处理乃至人工智能领域均掀起波澜。不同于普通的闲聊式机器人和任务型智能客服仅局限于固定场景，ChatGPT 具有相当丰富的知识储备，对于很多冷门的知识，它亦能对答如流，堪称当代“百晓生”。
因此，将语言模型与知识结合具有很高的研究价值，更强的知识性也标志着模型更加智能。
本文先讲述预训练语言模型与知识的关系，再阐述在对话系统中引入外部知识的原因和做法等方面，对基于知识的对话模型作简单综述。

#### 海纳百川——大语言模型也是知识库

&emsp;&emsp;一个知识库通常包含结构化或半结构化数据，例如实体、属性和关系，在人工构造知识库时往往也需要繁琐的工程技术和人工标注。
相比之下，语言模型是基于自然语言文本的统计模型，只需投入大量无标签文本语料就可以学习语言的规律和模式，然后利用这些规律和模式来生成文本或者回答关于文本的问题。

&emsp;&emsp;现在百花齐放的大语言模型如 BERT，GPT 等，都是在大量的文本数据上预训练过的，这已成为主流范式。维基百科，Reddit、知乎等论坛，推特、微博等媒体，都提供了海量的文本数据，语言模型把这些文本信息以参数化的形式存储。有研究工作以完形填空的形式对语言模型包含的知识进行探索，如下图所示，在实体-属性-关系形式的三元组数据集上验证，得出语言模型学习到并存储了一些事实知识。

<div align=center>
<img src="images/nlp_ds_1_mask.png" width="55%"/>
</div>


&emsp;&emsp;尽管语言模型不能像知识库那样提供明确的实体、属性和关系等结构化信息，但它们可以通过学习文本信息来获取知识，比如学习单词之间的语义关系，理解句子的结构和含义，识别实体和事件等。再将这样的知识库应用于下游任务，相比传统方法得到显著提升。

&emsp;&emsp;首先说明语言模型学到了哪些知识。语言模型从海量文本语料中学习了大量知识，语言模型除了学习到语言学知识外，还学到了大量世界性知识（或称事实知识）。

&emsp;&emsp;语言学知识主要包括单词之间的语义关系（比如词法、词性等），以及句子的结构和语法规则，从而理解自然语言。
同图像领域中低层的神经网络通常学习轮廓等低维通用特征一样，Transformer 为基础的大语言模型也是在低层存储这些语言学知识，这也是将预训练模型在下游任务上微调时将 Adapter 等结构加到上层网络的缘故。

&emsp;&emsp;世界性知识就是我们通常认定的一些客观事实，比如实体和事件的识别，语言模型可以学习到如何识别文本中的人名、地名、时间、事件等实体信息；也比如一些抽象的情感特征，文本分类和主题模型等，在新闻摘要、产品评论分类、社交媒体评论分类等任务上均可胜任。

&emsp;&emsp;目前的大语言模型在语言学知识上的表现已相当成熟，只需要借助少量的语料数据就能生成流畅连贯，语法正确的句子\cite{zhang2021you}，但是事实知识的学习是一个动态的过程，我们只能通过增加训练语料来让模型学习更多的世界知识，并且更新起来如果涉及模型微调相当麻烦，例如 ChatGPT 只能回答截止到训练时相关知识的问题，超过这一时间点就束手无策，这也是目前 ChatGPT 待解决的问题之一。

<div align=center>
<img src="images/nlp_ds_1_chatgpt.png" width="65%"/>
</div>

&emsp;&emsp;语言模型又是如何用作知识库呢？如前文所说，语言模型通常不能像传统的知识库一样提供结构化的实体、属性和关系等信息，但是，针对不同的子任务，只要设计良好的提示模板 Prompt，就可以提取出模型中的知识，提示学习 Prompt Learning 也是当前的主流方法。
关于将语言模型用作知识库 (Language Model as Knowledge Base) 这一范式，也有很多研究工作，文章提出模型在进行预训练时使用到的无监督文本语料非常庞大，因此很难保证模型将这些知识全部存储在参数中并且之后可以准确提取，于是尝试通过加入有关的上下文信息，在实体识别等任务上均提升了模型提取知识的准确率；而在中通过对prompt重新构造进行集成和组合，在问答测试等问题取得一定提升；通过在预训练时加入一个检索模块，使模型能够以更加具备解释性和模块化的来获取文本中的知识，将外部知识作为 Prompt 激发语言模型的知识能力。
虽然语言模型区别于传统的知识库，但是可以通过各种方式将其应用于类似于知识库的任务中，提供类似于知识库的信息。


#### 学海无涯——对话系话为何引入外部知识

&emsp;&emsp;对话系统引入外部知识可以生成信息更丰富的回复，利用外部知识库中的术语来生成更专业的表达式。外部知识对于消除模型幻觉 "Hallucination" 一直都具备重要意义，语言模型受限于训练语料本身存储的知识有限，对于很多场景的任务均需要外部知识辅助，不然本身容易生成具有事实性错误的内容。

&emsp;&emsp;此外，一些事实性知识在训练语料中出现频率较少，在生成回复时并不能很好利用，这时引入外部知识就可以作为一种提示，激发语言模型本身的知识用于回复生成。

<div align=center>
<img src="images/nlp_ds_klg_dial.png" width="60%"/>
</div>

#### 学以致用——对话模型如何利用外部知识？

&emsp;&emsp;许多研究人员致力于构建以知识为基础的对话系统。对话系统可以利用外部知识来增强其对话能力，使得其可以回答更加复杂和多样化的用户问题。其中的关键问题就在于如何把外部知识引入到对话模型中，一般来讲，包括将知识库以某种形式存储供模型调用，或者使用外部检索从海量文本中查找需要的知识。

<div align=center>
<img src="images/nlp_ds_klg.png" width="60%"/>
</div>

&emsp;&emsp;记忆网络是一种常用于对话模型中的网络结构，它可以用来增强对话模型记忆历史对话内容以及其他外部知识。知识为基础的系统使用记忆网络存储外部知识，生成器在生成阶段从中检索相关知识事实。对话模型可以使用记忆网络来检索外部知识库中的信息，从而使回复更具知识性，能够解决用户需求，也可以使用记忆网络来记录对话历史，以便更好地理解用户的意图和回答用户的问题，同时也避免在长对话历史场景中发生上下文不一致的问题，在特定话题以及人设对话场景中都非常重要，以更好地理解用户意图并生成自然回复。

&emsp;&emsp;对话模型也可以直接检索外部知识文档来辅助生成回复，外部知识文档可以是类似于 Wikipedia 这样的知识库，文档包含大量知识事实，但它们有一个缺点，即它们通常太长而无法从中检索有用的信息。
对话系统使用检索算法在外部知识库中寻找与用户输入相关的信息，检索算法可以使用基于文本相似度的方法，如词袋模型、tf-idf模型、文本向量化模型等；通常将对话上下文作为查询语句进行检索，也有工作进行一种后验选择，先将对话上下文作为输入并生成回复，再利用回复和上下文一块来检索文档。这也意味着需要对生成的多个回复进行过滤和排序，返回最相关和准确的信息。

&emsp;&emsp;由于文本知识检索不需要改变模型参数就可以结合最新知识，这也是非常有前景的研究方向；区别于传统的文本检索，

&emsp;&emsp;知识图是外部信息的另一个来源，由于其实体-属性-关系的结构化特性，它在以知识为基础的系统中越来越受欢迎。
随着图神经网络的发展，很多研究都侧重从知识图谱中获取更强的信息表征然后应用到对话任务。知识图谱为模型提供了全面、丰富的实体特征和关系，模型在存储知识时也更倾向于这种实体关系的映射，所以知识图谱往往更容易增强了模型的知识性和鲁棒性，而对话任务由于经常有多轮交互，涉及通过上下文在知识图谱上转移到更有意义的节点。

#### 大道行思——知识对话模型的未来展望

&emsp;&emsp;随着 ChatGPT 的大获成功，知识对话模型也受到越来越多的关注，在很多方向上都具备很高的研究价值：可以将图像、视频等多模态信息融入对话模型中，可以进一步提高对话的自然度和实用性；未来的知识对话模型将更加注重对用户个性化需求的满足，包括对用户的历史对话记录、兴趣爱好等信息的建模和利用，以实现更自适应和个性化的对话服务；此外，微软最近将 ChatGPT 与 Bing 结合一改搜索引擎的范式，通过对话查询的形式返回最新的网页链接，这又是对传统文档检索的一次突破；未来对话模型也会更注重对话策略和生成算法的智能优化，以提高对话的质量和效率，包括如何更好地利用对话历史和上下文信息，以及如何更好地生成自然、连贯的对话文本。
