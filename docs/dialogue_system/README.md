# 对话系统 Dialogue System

## 基于知识的对话大模型

&emsp;&emsp;ChatGPT 的横空出世，在整个自然语言处理乃至人工智能领域均掀起波澜。不同于普通的闲聊式机器人和任务型智能客服仅局限于固定场景，ChatGPT 具有相当丰富的知识储备，对于很多冷门的知识，它亦能对答如流，堪称当代“百晓生”。
因此，将语言模型与知识结合具有很高的研究价值，更强的知识性也标志着模型更加智能。
本文先讲述预训练语言模型与知识的关系，再阐述在对话系统中引入外部知识的原因和做法等方面，对基于知识的对话模型作简单综述。

#### 海纳百川——大语言模型也是知识库

&emsp;&emsp;一个知识库通常包含结构化或半结构化数据，例如实体、属性和关系，在人工构造知识库时往往也需要繁琐的工程技术和人工标注。
相比之下，语言模型是基于自然语言文本的统计模型，只需投入大量无标签文本语料就可以学习语言的规律和模式，然后利用这些规律和模式来生成文本或者回答关于文本的问题。

&emsp;&emsp;现在百花齐放的大语言模型如 BERT，GPT 等，都是在大量的文本数据上预训练过的，这已成为主流范式。维基百科，Reddit、知乎等论坛，推特、微博等媒体，都提供了海量的文本数据，语言模型把这些文本信息以参数化的形式存储。有研究工作以完形填空的形式对语言模型包含的知识进行探索，如下图所示，在实体-属性-关系形式的三元组数据集上验证，得出语言模型学习到并存储了一些事实知识。

<div align=center>
<img src="images/nlp_ds_1_mask.png" width="55%"/>
</div>


&emsp;&emsp;尽管语言模型不能像知识库那样提供明确的实体、属性和关系等结构化信息，但它们可以通过学习文本信息来获取知识，比如学习单词之间的语义关系，理解句子的结构和含义，识别实体和事件等。再将这样的知识库应用于下游任务，相比传统方法得到显著提升。

&emsp;&emsp;首先说明语言模型学到了哪些知识。语言模型从海量文本语料中学习了大量知识，语言模型除了学习到语言学知识外，还学到了大量世界性知识（或称事实知识）。

&emsp;&emsp;语言学知识主要包括单词之间的语义关系（比如词法、词性等），以及句子的结构和语法规则，从而理解自然语言。
同图像领域中低层的神经网络通常学习轮廓等低维通用特征一样，Transformer 为基础的大语言模型也是在低层存储这些语言学知识，这也是将预训练模型在下游任务上微调时将 Adapter 等结构加到上层网络的缘故。

&emsp;&emsp;世界性知识就是我们通常认定的一些客观事实，比如实体和事件的识别，语言模型可以学习到如何识别文本中的人名、地名、时间、事件等实体信息；也比如一些抽象的情感特征，文本分类和主题模型等，在新闻摘要、产品评论分类、社交媒体评论分类等任务上均可胜任。

&emsp;&emsp;目前的大语言模型在语言学知识上的表现已相当成熟，只需要借助少量的语料数据就能生成流畅连贯，语法正确的句子\cite{zhang2021you}，但是事实知识的学习是一个动态的过程，我们只能通过增加训练语料来让模型学习更多的世界知识，并且更新起来如果涉及模型微调相当麻烦，例如 ChatGPT 只能回答截止到训练时相关知识的问题，超过这一时间点就束手无策，这也是目前 ChatGPT 待解决的问题之一。

<div align=center>
<img src="images/nlp_ds_1_chatgpt.png" width="65%"/>
</div>

&emsp;&emsp;语言模型又是如何用作知识库呢？如前文所说，语言模型通常不能像传统的知识库一样提供结构化的实体、属性和关系等信息，但是，针对不同的子任务，只要设计良好的提示模板 Prompt，就可以提取出模型中的知识，提示学习 Prompt Learning 也是当前的主流方法。
关于将语言模型用作知识库 (Language Model as Knowledge Base) 这一范式，也有很多研究工作，文章提出模型在进行预训练时使用到的无监督文本语料非常庞大，因此很难保证模型将这些知识全部存储在参数中并且之后可以准确提取，于是尝试通过加入有关的上下文信息，在实体识别等任务上均提升了模型提取知识的准确率；而在中通过对prompt重新构造进行集成和组合，在问答测试等问题取得一定提升；通过在预训练时加入一个检索模块，使模型能够以更加具备解释性和模块化的来获取文本中的知识，将外部知识作为 Prompt 激发语言模型的知识能力。
虽然语言模型区别于传统的知识库，但是可以通过各种方式将其应用于类似于知识库的任务中，提供类似于知识库的信息。


#### 学海无涯——对话系话为何引入外部知识

&emsp;&emsp;对话系统引入外部知识可以生成信息更丰富的回复，利用外部知识库中的术语来生成更专业的表达式。外部知识对于消除模型幻觉 "Hallucination" 一直都具备重要意义，语言模型受限于训练语料本身存储的知识有限，对于很多场景的任务均需要外部知识辅助，不然本身容易生成具有事实性错误的内容。

&emsp;&emsp;此外，一些事实性知识在训练语料中出现频率较少，在生成回复时并不能很好利用，这时引入外部知识就可以作为一种提示，激发语言模型本身的知识用于回复生成。

<div align=center>
<img src="images/nlp_ds_klg_dial.png" width="60%"/>
</div>

#### 学以致用——对话模型如何利用外部知识？

&emsp;&emsp;许多研究人员致力于构建以知识为基础的对话系统。对话系统可以利用外部知识来增强其对话能力，使得其可以回答更加复杂和多样化的用户问题。其中的关键问题就在于如何把外部知识引入到对话模型中，一般来讲，包括将知识库以某种形式存储供模型调用，或者使用外部检索从海量文本中查找需要的知识。

<div align=center>
<img src="images/nlp_ds_klg.png" width="60%"/>
</div>

&emsp;&emsp;记忆网络是一种常用于对话模型中的网络结构，它可以用来增强对话模型记忆历史对话内容以及其他外部知识。知识为基础的系统使用记忆网络存储外部知识，生成器在生成阶段从中检索相关知识事实。对话模型可以使用记忆网络来检索外部知识库中的信息，从而使回复更具知识性，能够解决用户需求，也可以使用记忆网络来记录对话历史，以便更好地理解用户的意图和回答用户的问题，同时也避免在长对话历史场景中发生上下文不一致的问题，在特定话题以及人设对话场景中都非常重要，以更好地理解用户意图并生成自然回复。

&emsp;&emsp;对话模型也可以直接检索外部知识文档来辅助生成回复，外部知识文档可以是类似于 Wikipedia 这样的知识库，文档包含大量知识事实，但它们有一个缺点，即它们通常太长而无法从中检索有用的信息。
对话系统使用检索算法在外部知识库中寻找与用户输入相关的信息，检索算法可以使用基于文本相似度的方法，如词袋模型、tf-idf模型、文本向量化模型等；通常将对话上下文作为查询语句进行检索，也有工作进行一种后验选择，先将对话上下文作为输入并生成回复，再利用回复和上下文一块来检索文档。这也意味着需要对生成的多个回复进行过滤和排序，返回最相关和准确的信息。

&emsp;&emsp;由于文本知识检索不需要改变模型参数就可以结合最新知识，这也是非常有前景的研究方向；区别于传统的文本检索，

&emsp;&emsp;知识图是外部信息的另一个来源，由于其实体-属性-关系的结构化特性，它在以知识为基础的系统中越来越受欢迎。
随着图神经网络的发展，很多研究都侧重从知识图谱中获取更强的信息表征然后应用到对话任务。知识图谱为模型提供了全面、丰富的实体特征和关系，模型在存储知识时也更倾向于这种实体关系的映射，所以知识图谱往往更容易增强了模型的知识性和鲁棒性，而对话任务由于经常有多轮交互，涉及通过上下文在知识图谱上转移到更有意义的节点。

#### 大道行思——知识对话模型的未来展望

&emsp;&emsp;随着 ChatGPT 的大获成功，知识对话模型也受到越来越多的关注，在很多方向上都具备很高的研究价值：可以将图像、视频等多模态信息融入对话模型中，可以进一步提高对话的自然度和实用性；未来的知识对话模型将更加注重对用户个性化需求的满足，包括对用户的历史对话记录、兴趣爱好等信息的建模和利用，以实现更自适应和个性化的对话服务；此外，微软最近将 ChatGPT 与 Bing 结合一改搜索引擎的范式，通过对话查询的形式返回最新的网页链接，这又是对传统文档检索的一次突破；未来对话模型也会更注重对话策略和生成算法的智能优化，以提高对话的质量和效率，包括如何更好地利用对话历史和上下文信息，以及如何更好地生成自然、连贯的对话文本。

## 对话模型中的事实错误

&emsp;&emsp;随着 ChatGPT 的横空出世，智能对话大模型俨然已成为 AI 发展的焦点，更是在整 个自然语言处理 (NLP) 领域掀起了一阵海啸。自去年席卷全球以来便引起各行各业空前 的热度，数亿用户纷纷惊叹于 ChatGPT 的强大功能，思考其背后关键技术革新，也关注 当前 ChatGPT 仍存在哪些缺陷，除了巨量数据资源的耗费需求，无法与时俱进关联最新 信息等之外，一个引人瞩目的问题就是 ChatGPT 交互中仍会生成不少的事实性错误，对 一些老幼皆知的简单问题也会一本正经的胡说八道，如下图所示

<div align=center>
<img src="./../images/nlp_ds_chatgpt1.png" width="60%"/>
</div>

&emsp;&emsp;这种事实性错误的存在对产品的广泛落地不可谓不致命，假设应用于医学，金融等非闲 聊式场景，这些潜在风险就可能会造成经济损失或威胁生命安全，因此消除大模型中的 事实错误成为了工业界和学术界的共同需求。

&emsp;&emsp;首先明确一则术语，在 NLP 论文中，这种事实性错误一般统称为“幻觉”(Hallucination)，顾名思义，该术语最早用于图像合成等领域，直到后来描述诸如图像检测时检 测到虚假或错误目标等现象，才沿用至自然语言生成 (NLG) 任务，指模型生成自然流畅，语法正确但实际上毫无意义且包含虚假信息即事实错误的文本，以假乱真，就像人产生的幻觉一样。

&emsp;&emsp;本文主要参考港科大的综述文章，全部采用模型“幻觉”来指代前文所述的事实性错误，先简述下 NLG 生成“幻觉”文本的成因，接着详细介绍对话任务中的“幻觉” 现象，针对对话任务的“幻觉”评估和解决方法等。

#### 自然语言生成的“幻觉”成因

&emsp;&emsp;此前已有相关工作将自然文本生成中的幻觉问题分为内部幻觉 (intrinsic hallucination) 和外部幻觉 (extrinsic hallucination) 两大类。所谓内部幻觉，是指生成的文本与给 定源文本出现不忠实 (unfaithfulness) 或不一致 (inconsistency) 的现象，这种问题常见于文本摘要任务中，生成的摘要与原文不一致。另一种外部幻觉，则是指生成内容在源文本 中并未提及，我们虽然不能找出相关证据，但也不能断言这就是错误的，这就涉及到语言模型本身的事实性知识 (factual knowledge) 或世界知识 (real-world knowledge)。下面对 这些幻觉的成因进行分析。

&emsp;&emsp;虽然两类幻觉表现上有着明显区别，但成因却较为类似，根据以下因果图即可简单分析

<div align=center>
<img src="./../images/nlp_ds_causal.png" width="60%"/>
</div>

&emsp;&emsp;生成的文本 $Y$ 由源文本 $X$ 和语言模型里的先验知识 $K$ 共同决定，由于一般认为给定的源文本都是事实正确的 ground-truth，所以出现的幻觉一般都会归结于语言模型本身包含了错误事实。语言模型中的先验知识都来自于训练语料，用于训练语言模型的大数据语料库在收集时难免会包含一些错误的信息，这些错误知识都会被学习，存储在模型参数中，相关研究表明模型生成文本时会优先考虑自身参数化的知识，所以更倾向生成幻觉内容，这也是自然语言生成任务中大部分幻觉的来源，本文后续讨论的幻觉默认基于这个原因。

&emsp;&emsp;另一方面，模型训练和推理时的差异，也是导致推理时更容易生成幻觉的原因之一。训练中通常用最大似然估计的做法训练解码器，并且是以 ground-truth 作为后续预测 token 的前缀输入，而解码时则根据历史序列生成来预测下一个 token。通过改变解码策略，也能一定程度上增强生成文本的事实性。

#### 对话系统中的“幻觉”现象

&emsp;&emsp;随着对话大模型 ChatGPT 的巨大成功，对话系统的“幻觉”现象也成为热门的研究方向。
构建对话系统需要根据用户话语和对话历史生成流畅连贯，与对话历史保持一致，且满足用户对话需求的合理回复。
对话任务分为任务型对话 (Task-Orientated Dialogue) 和开放域对话 (Open-Domain Dialogue)，任务型对话在很多场景的自动客服上已广泛应用，评测也相对容易，研究难点主要集中在类似 ChatGPT 这种开放域对话模型上，虽然是用于闲聊场景，但是需要满足各种用户能够提出的问题，解决各类任务，由于针对一条用户话语的回复并不唯一，也很难制定一个完善的评价体系。
因此开放域对话中的幻觉研究也更具有挑战性，在对话模型的研究中，用来描述这些文本生成的幻觉问题则有一个更常见的术语，“不一致” (inconsistency)。

&emsp;&emsp;对话回复的不一致也可分为两类，模型自身不一致 (self-inconsistency) 和外部不一致 (external inconsistency)。
自身不一致是指模型生成的回复与对话历史或与自身已生成回复相矛盾，属于前文所提及的内部幻觉；与对话历史的不一致性问题一般来自于历史信息的遗忘，包含与已生成文本相矛盾，这是人设 (persona) 对话中常见的问题，赋予系统一个固定角色，在聊天过程中模型的人设信息会发生变化，如下图所示

<div align=center>
<img src="images/nlp_ds_persona.png" width="40%"/>
</div>

&emsp;&emsp;而另一种外部不一致的问题，则是对话系统为了生成角色一致且信息丰富的回复，会将包含显式角色信息的外部数据引入系统以辅助模型生成，例如 PersonaChat 数据集中每个对话都伴随着人物描述，通过以角色描述为条件来增强角色一致性。
尽管 PersonaChat 已经在不少方法上取得显著效果，但进一步研究不依赖于给定角色描述的方法依然是有必要的，因为单靠文本描述很难覆盖一个角色的所有特征。

&emsp;&emsp;开放域知识对话要求模型在外部知识图或知识语料库的帮助下，将外部知识视为源文本的一部分，生成与知识描述一致并且完成对话任务的回复。
因此知识对话中的“幻觉”或“不一致”则聚焦于回复与外部知识的关系，比如一种常见的问题就是生成回复包含与所提供的外部知识不一致（图4）。

<div align=center>
<img src="images/nlp_ds_incon.png" width="40%"/>
</div>

&emsp;&emsp;由于世界知识数量庞大且不断更新，知识对话的很难保持事实正确性 (factual correctness)，也就是前文所说的外在幻觉，它可能是真实的但难以验证。
知识对话涉及两个步骤，首先需要用一个检索模型根据上下文对话来检索所需要的相关知识，然后将检索的知识作为输入文本生成回复，对这两个阶段进行优化都可以显著提高对话的事实正确性。
文章构造了一个专门用于知识对话事实一致性检测的数据集 Conv-FEVER，通过改编 Wizard-of-Wikipedia 创建，由事实一致和不一致的回复组成，该工作将回复与外部知识的关系分为三类：第一种是非验证性回复，即生成的回复只是做了简短闲聊，并未涉及任何外部知识，也就不需要来验证与知识的一致性；
第二种是事实一致性回复，即回复与外部知识高度相关一致，利用外部知识完成对话任务，这是最理想的状态；
第三种则是外部幻觉性回复，即回复中的知识与提供的外部知识信息不一致，由于外部知识也是检索而来，我们无法断言回复就一定发生了事实错误，这就需要借助“世界知识”来判断；这三种分类基本涵盖了大部分知识对话可能产生的幻觉现象，利用该数据集，可训练分类器以检测与所提供的知识相关的事实不一致的回复。


#### 对话系统“幻觉”的评估方法

&emsp;&emsp;对话系统，尤其是开放域对话，模型幻觉的评估仍是一个悬而未决的难题，截至目前也没有标准指标。
对话回复往往根据事实一致性或事实正确性进行评估，整体有三条思路，分别是基于统计，模型和人工的评估方法。


&emsp;&emsp;基于统计方法的评估：最简单的幻觉评估方法是直接计算生成文本和参考目标文本之间的词汇匹配度，例如 ROUGE，BLEU 等，F1 分数结合了生成文本和目标文本的精确率和召回率，能更好的反应其匹配关系。额外使用源文本是因为不能保证输出目标文本包含输入源文本中可用的完整信息集。 
然而开放域对话任务的回复往往不唯一，输入与输出是一对多的映射，而实际应用中，覆盖所有可能的输出几乎不可能，这也给目标参考的选择造成很大困难。
在知识对话中，假设给定的外部知识是事实正确的，那么我们则更关注生成回复与依赖的外部知识间的一致性，相关工作提出 Knowledge F1 指标用于测量生成回复数据集收集期间参考的真实知识之间的重叠，非常适用于知识对话场景。

&emsp;&emsp;基于模型方法的评估：模型评估方法主要基于自然语言推理 (Natural Language Inference, NLI)，即判断一项假设（即生成文本）是否蕴含于前提（即参考文本），基于 NLI 的指标将幻觉分数定义为源文本与生成文本之间的蕴含概率，也称为生成文本蕴含、中立和矛盾的次数百分比；当然，这也意味着需要先收集相关蕴含关系的数据集来训练这样一个判别模型，当前已有不少工作工作发布额外的一致性数据集在基于 NLI 的方法上训练可学习的评估指标，如前文提到的 Conv-FEVER 数据集；在知识对话中，还有工作提出借助问答系统的 $Q^2$ 方法来判断回复与外部知识的一致性。

&emsp;&emsp;基于模型的评估方法整体上比词级别的统计方法要适用得多，然而文本蕴含模型只能返回一个分数，无法定位具体生成文本的哪些部分是错误的，从消除事实错误的角度讲，还是应该更关注细粒度，可以定位到词级别的评估方法。

&emsp;&emsp;基于人工的评估：由于当前模型幻觉问题自动评估方法仍不完善，无论是通用的文本生成任务还是对话系统，人工评估仍然是最常用的方法之一，通常有对幻觉文本进行评分或与 ground-truth 直接比较两种方法，但都不可避免的会增加研究成本，一种比较折中的方法是，先让人工评估者在某个范围内对回复中的幻觉现象进行评分，然后提出改进一些现有的自动评估方法，只要该自动评估指标与人类评估打分趋势保持一致，就可以认为是个可行的评估方法。


#### 对话模型“幻觉”的解决办法

&emsp;&emsp;前文已分析过，造成模型生成幻觉文本的首要因素是训练数据，那么构造干净的数据集，进行数据去噪显然是一条可行的方法。
由于预训练数据多为网上收集的句子，一般都需要进行去噪操作，修改语法，指代不明或事实错误，确保语言模型能学习到事实准确的知识，另外也可以用 Wikipedia 这样的知识库或其他三元组表示的知识来对语言模型的进行知识增强，这些数据都是公认的包含世界知识的准确数据，对于降低模型幻觉的干扰有很大帮助；
在对话微调阶段，可以先用前文的测量指标对回复进行评估打分，然后过滤掉那些低质量不可信的回复，人工标注的样本也需要严格的筛选，确保模型生成与外部知识一致，并且保留对话特性，能完成应用场景对话任务的可靠回复。

&emsp;&emsp;数据方法涉及到人工构造，也就意味着成本较高，所以学术上更加关注使用其他方法，这里统称为模型方法，是因为区别于数据方法，其他方法都是对模型的架构、训练或推断来进行优化的。
对开放域对话任务而言，我们往往希望能控制生成回复的偏好，使其具备某种特性，以手动提供的可控 token 或自动预测的方式加到输入上，来提高生成回复的可信度，例如训练一个 token 来控制生成回复偏向第一人称的主观表达还是第三人称的客观陈述，然后在测试阶段通过手动赋值就可以控制回复的偏好了，这对于人设对话是很有参考价值的，如果将人设特征如性格、年龄、性别、职业、爱好等都设置成可控 token，那么只需要手动设置，就可以构建出各式各样的虚拟角色了。

&emsp;&emsp;另外在知识对话中，模型幻觉很大程度来源之一是外部知识选择不正确，因此用更强的检索模型搜索知识，返回更加有用的知识，也是消除对话回复幻觉的有效途径之一。
当前 ChatGPT 并不具备检索能力，其模型内部的隐式知识已然非常强大，一旦可以进行检索，结合网络上海量数据，就可以做到实时学习，并更新模型内部过期的知识，这对模型效果的进一步提升也是相当可观的。

最后再来调戏一把 ChatGPT。

<div align=center>
<img src="images/nlp_ds_chatgpt.png" width="60%"/>
</div>

<div align=center>
<img src="images/nlp_ds_chatgpt2.png" width="60%"/>
</div>

&emsp;&emsp;本文受启发于 ChatGPT 的事实错误问题，从文本生成的幻觉问题入手，对当前对话大模型的幻觉现象，评估以及解决方法进行了简单综述，模型幻觉仍是对话大模型的发展瓶颈，本文也不难看出，无论是幻觉评估还是解决，受限于对话任务本身的难度，都还是任重而道远，不过我们有理由相信，一旦解决了模型幻觉这一重大难题，那通用大模型也就离真正的人工智能不过咫尺之遥了。
