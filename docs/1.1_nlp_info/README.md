# 1 自然语言与信息论 Natural Language and Information Theory

&emsp;&emsp;自然语言处理（Natural Language Processing，NLP）主要研究如何让计算机理解、分析、处理和生成人类自然语言，从而实现与人类的自然语言交互，是实现人工智能的关键。作为人类信息交互的重要载体，我们需要一种数学方法来量化自然语言的信息，随着香农 (Cluade Shannon) 1948 年在他的著名论文《通信的数学原理 (A Mathematic Theory of Communication)》[[1]](./5.1_refer/README.md) 提出 “信息熵 (Information Entropy)” 的概念，才解决了信息度量的问题，搭起数学和信息的桥梁。

&emsp;&emsp;香农这篇信息论的开山之作，主要讲离散和连续信号在通信时的数学理论，对信号包含的信息价值进行量化。那么何为信息呢？当一个事件具有多种可能得状态时，我们将该事件属于哪种状态的不确定性 (Uncertainty) 称为 “熵 (Entropy)”，而信息就是用来消除事件不确定性的。举个简单的例子，某个通信场景传递 “明天是否下雨” 的消息，在信道中发送一些约定的符号，比如离散字母 (A 代表下雨，B 代表不下雨)，接收者通过接收 A 或 B 就能获取第二天天气的信息。这就是信息的意义。

## 1.1 熵的物理意义 Entropy in Physics

&emsp;&emsp;本节重点介绍熵、条件熵、相对熵三者的定义和推导过程，建立数学和信息度量的联系。

https://www.zhihu.com/question/23763638

#### 熵的统计力学定义

&emsp;&emsp;在讨论信息熵之前，我们先谈谈熵的物理意义。熵的概念最早起源于热⼒学，用于描述系统的⽆序或混乱程度，这种混乱程度或无序性代表了我们对系统的未知性或不确定程度。使用统计力学对熵进行量化，可以这样理解其含义：把 $N$ 个完全相同的物体分到若⼲个箱⼦中，使得第 $i$ 个箱⼦有 $n_i$ 个物体。考虑把物体分配到箱⼦中不同⽅案的数量。
有 $N$ 种⽅式选择第⼀个物体，有 $N-1$ 种⽅式选择第⼆个物体，依次类推，总共有 $N!$ 种⽅式把 $N$ 个物体分配到箱⼦中。然⽽，我们不想区分每个箱⼦内物体的重新排列。
在第 $i$ 个箱⼦中，有 $n_i!$ 种⽅式对物体重新排序，因此把 $N$ 个物体分配到箱⼦中的总⽅案数量为

$$W=\frac{N!}{\prod_{i}n_i!}\tag1$$

这被称为乘数 (multiplicity)，熵被定义为通过适当参数放缩后的对数乘数，即

$$H=\frac{1}{N}\ln W=\frac{1}{N}\ln N!-\frac{1}{N}\sum_{i}\ln n_i!\tag2$$

&emsp;&emsp;现在考虑极限 $N\rightarrow\infty$，并且保持⽐值 $\frac{n_i}{N}$ 固定，使⽤ Stirling 估计

$$\ln N!\simeq N\ln N-N\tag3$$

可得
$$H=-\lim_{N\rightarrow\infty}\sum_{i}\left ( \frac{n_i}{N} \right )\ln \left ( \frac{n_i}{N} \right )=-\sum_i p_i\ln p_i\tag4$$
利用 $\sum_in_i=N$ 很容易推导出公式 (2)，这⾥，$p_i=\lim_{N\rightarrow\infty}\left ( \frac{n_i}{N} \right )$ 是⼀个物体被分配到第 $i$ 个箱⼦的概率，箱⼦中物体的具体分配⽅案被称为微观状态 (microstate)，对应概率中某个事件处于哪种状态；箱子中物体数的分布占比 $\frac{n_i}{N}$ 被称为宏观状态 (macrostate)，用于描述事件的概率分布。

&emsp;&emsp;如果用离散随机变量 $X$ 表示第 $i$ 个箱⼦的状态 $x_i$，其中 $P(X=x_i)=p_i$。这样，随机变量 $X$ 的熵可表示为

$$H[p]=-\sum_{i}p(x_i)\ln p(x_i)\tag5$$

&emsp;&emsp;如果分布 $p(x_i)$ 不是等概率分布，特别是，有几个值的概率比较高的话，熵就会相对较低。当 $p_i=1$ 且所有的 $p_{j\ne i}=0$ 时，熵取得最⼩值 0；可以证明当所有 $p(x_i)$ 都相等且 $p(x_i)=\frac{1}{M}$ 时，熵取得最⼤值，其中，$M$ 是状态 $x_i$ 的总数，此时对应熵值为 $H=\ln M$。

> 注：关于最大熵值 $H=\ln M$ 的推导可参考[附录2](./5.2_math/README.md)。

&emsp;&emsp;我们把熵的定义扩展到连续空间，假设变量 $x$ 的概率分布为 $p(x)$，将 $x$ 切分成宽度为 $\Delta$ 的箱⼦。根据均值定理 (mean value theorem)，对于每个这样的箱⼦，存在⼀个值 $x_i$ 使得

$$\int_{i\Delta}^{(i+1)\Delta}p(x)\mathrm dx=p(x_i)\Delta\tag{6}$$

&emsp;&emsp;只要 $x$ 落在第 $i$ 个箱⼦中，我们就把 $x$ 赋值为 $x_i$，观察到 $x_i$ 的概率为 $p(x_i)\Delta$，即使用概率 $p(x_i)\Delta$ 代表落在区间 $[{i\Delta},{(i+1)\Delta}]$ 上的概率，这就变成了离散的分布，这种情形下熵的形式为

$$H_\Delta=-\sum_{i}p(x_i)\Delta\ln (p(x_i)\Delta)=-\sum_{i}p(x_i)\Delta\ln p(x_i)-\ln \Delta\tag{7}$$

&emsp;&emsp;考虑极限 $\Delta\rightarrow0$，公式 (7) 就变成 $-\int p(x)\ln p(x)\mathrm d x$，这被称为微分熵 (differential entropy)。同时被忽略项 $-\ln \Delta$ 正是熵的离散形式与连续形式的差，在 $\Delta\rightarrow\infty$ 时会发散，这就反映出将熵的离散形式转换为连续形式需要大量比特位，对应我们在计算机里使用浮点数存储连续型变量需要大量额外内存。

> 注：微分熵最大化时对应 $p(x)$ 为高斯分布，证明过程见[附录2](./5.2_math/README.md)。

#### 条件熵

&emsp;&emsp;假设有⼀个联合概率分布 $p(x,y)$，从这个概率分布中采样一组 $x$ 和 $y$。如果 $x$ 的值已知，那么确定 $y$ 值就要用到条件概率分布 $p(y|x)$，定义此时给定 $x$ 的情况下，$y$ 的条件熵 (conditional entropy) 为

$$H[y|x]=-\int\int p(x,y)\ln p(y|x)\mathrm dy \mathrm dx\tag{8}$$
条件熵满⾜如下关系

$$H[x,y]=H[y|x]+H[x]\tag{9}$$
其中，$H[x,y]$ 是 $p(x,y)$ 的微分熵，$H[x]$ 是边缘分布 $p(x)$ 的微分熵。$y$ 的条件熵也就是⽤来确定 $y$ 的平均附加信息。因此，描述 $x$ 和 $y$ 所需的信息是描述 $x$ 所需的信息，再加上给定 $x$ 的情况下确定 $y$ 所需的额外信息。

#### 相对熵

&emsp;&emsp;接下来介绍相对熵 (relative entropy)，相对熵最早用于度量两个信号的差异。而如果用来度量事件的信息，那么相对熵实际上就是在衡量两个概率分布函数的相似性，相对熵越大，两个随机分布的差异越大，反之越小。考虑某个未知的分布 $p(x)$，以及使⽤某个分布 $q(x)$ 对它进⾏近似。如果我们使⽤ $q(x)$ 表示信息，由于不是真实分布，我们就需要⼀些附加的信息，这部分信息就是用来度量两个分布的平均信息差，也被称为分布 $p(x)$ 和 $q(x)$ 之间的相对熵或 Kullback-Leibler 散度（KL 散度），为

$$\mathrm {KL}(p||q)=-\int p(x)\ln q(x)\mathrm d x-\left ( -\int p(x)\ln p(x)\mathrm d x \right )=-\int p(x)\ln\frac{ q(x)}{p(x)}\mathrm d x\tag{10}$$

&emsp;&emsp;KL 散度满足 $\mathrm {KL}(p(x)||q(x))\geq0$，当且仅当 $p(x)=q(x)$ 时等号成⽴。使用 Jensen 不等式即可证明

$$\mathrm {KL}(p||q)=-\int p(x)\ln\frac{ q(x)}{p(x)}\mathrm d x\geq-\ln \int q(x)\mathrm d x\tag{11}$$

&emsp;&emsp;再结合归一化 $\int q(x)\mathrm d x=1$，可得 $\mathrm {KL}(p(x)||q(x))\geq0$。实际上，$-\ln x$ 是严格凸函数，因此只有 $q(x)=p(x)$ 对于所有 $x$ 都成⽴时，等号才成⽴，可以看出，如果我们使用不同于真实概率分布的概率密度估计，那么一定会增加等于两个分布 KL 散度的额外信息量，这在通信应用中就不可避免的造成了编码效率损失。

&emsp;&emsp;另外，相对熵是不对称的，也就是 $\mathrm {KL}(p||q)\ne\mathrm {KL}(q||p)$，这是因为实数在衡量差异时有严格的参考，而概率分布只能以自身为参考来衡量和另一个分布的差异。有时不是很方便，为了使其对称，Jensen 和 Shannon 提出一种新的计算方式

$$\mathrm{JS}(p||q)=\frac{1}{2}[\mathrm {KL}(p||q)+\mathrm {KL}(q||p)]\tag{12}$$

<!-- &emsp;&emsp;假设数据通过未知分布 $p(x)$ ⽣成，对 $p(x)$ 建模可以使⽤⼀些参数分布 $q(x|\theta)$ 来近似这个分布。$q(x|\theta)$ 由可调节的参数 $\theta$ 控制（例如⼀个⾼斯分布）。⼀种确定 $\theta$ 的⽅式是最⼩化 $p(x)$ 和 $q(x|\theta)$ 关于 $\theta$ 的 KL 散度。虽然我们不知道 $p(x)$，但是，可以从观察到的服从分布 $p(x)$ 的有限训练数据 $\left \{ x_n\right \}$ 通过这些数据的有限加和近似 $p(x)$ 的期望，即

$$\mathrm {KL}(p||q)\simeq \frac{1}{N}\sum_{n=1}^N\left \{ -\ln q(x_n|\theta) +\ln p(x_n)\right \}\tag{13}$$

&emsp;&emsp;第⼀项是使⽤训练集估计的分布 $q(x|\theta)$ 下 $\theta$ 的负对数似然函数，第二项与 $\theta$ 无关，所以最⼩化 KL 散度等价于最⼤化 $\theta$ 似然函数。后面在第十章贝叶斯网络中将会见到一种引入贝叶斯定理，使用一个参数分布 $q(\theta|x)$ 去逼近 $\theta$ 的先验分布 $p(\theta)$ 的方法，这种方法对于抑制网络过拟合，提高模型泛化能力有重要作用。 -->

#### 互信息

&emsp;&emsp;现在考虑由联合概率分布 $p(x,y)$ 给出的两个随机变量 $x$ 和 $y$ 组成的数据集，如果两个变量是相互独⽴的，那么联合分布可以分解为边缘分布的乘积 $p(x,y)=p(x)p(y)$。如果变量不完全独⽴，我们也可以通过考察联合概率分布与边缘概率分布乘积之间的 KL 散度来判断它们是否近似于相互独⽴。此时，KL 散度为

$$I[x,y]=\mathrm {KL}(p(x,y)||p(x)p(y))=-\int\int p(x,y)\ln \left ( \frac{p(x)p(y)}{p(x,y)} \right )\mathrm d x\mathrm dy\tag{13}$$

&emsp;&emsp;这被称为 $x$ 和 $y$ 的互信息 (mutual information)，就是已知变量 $x$ 的前提下，$y$ 的信息熵与条件熵的差异。在 $p(x,y)=p(y|x)p(x)$ 时，互信息和条件熵之间的关系为

$$\begin{align} I[x,y]&=-\int\int p(x,y)\ln p(x)\mathrm d x\mathrm dy-\int\int p(x,y)\ln p(y)\mathrm d x\mathrm dy+\int\int p(x,y)\ln p(x,y)\mathrm d x\mathrm dy\\ &=-\int\int p(x,y)\ln p(y)\mathrm d x\mathrm dy+\int\int p(x,y)\ln p(y|x)\mathrm d x\mathrm dy\\ &=H[y]-H[y|x]\\ \end{align}\tag{14}$$

&emsp;&emsp;可以看出 $H[y] \le H[y|x]$ 恒成立，也就是在给定 $x$ 的前提下，$y$ 的不确定性一定不变或减小，
根据贝叶斯定理，我们可以把 $p(y)$ 看成 $y$ 的先验概率分布，把 $p(y|x)$ 看成观察到新数据 $x$ 之后的后验概率分布，互信息就表示⼀个新的观测数据 $x$ 造成的随机变量 $y$ 的不确定性的减⼩；当 $x$ 和 $y$ 完全无关时，$I[x,y]=0$。

## 1.2 信息熵和不确定性 Information Entropy and Uncertainty

&emsp;&emsp;一条信息的信息量与信息对应事件的不确定性有直接联系，当我们对一件事很不确定或者一无所知，此时有人告知我们这件事情要发生，我们将获得大量信息；如果已知一件事情很有可能发生，此时有人告诉我们这件事发生了，这时所获取的信息量相比前者就会小很多。因此可以认为，**一条信息的价值取决于其消除的不确定性，消除的不确定性越高，熵减越多，信息的价值就越大**。
上一节公式推导非常枯燥繁琐，本节就用四个有趣的例子更生动形象的说明信息是如何实现熵减从而消除不确定性的。

https://www.zhihu.com/question/22178202

https://blog.codinglabs.org/articles/simple-explain-of-amtoc.html

https://zhuanlan.zhihu.com/p/346611927

#### 示例一：猜小球颜色 - 信息熵的基本性质有哪些

&emsp;&emsp;考虑一个猜小球颜色的例子，盒子里有除颜色外其余都相同的四个小球，有黑白两种颜色，随机抓取一个小球，我们和一群观众猜测该小球颜色。如果我们对盒子里两种颜色的小球数量没任何先验了解，很自然地就会认为抓到小球的颜色为黑和白的概率均为 $\frac{1}{2}$ ；但假如所有人都已知该盒子里有三个黑球和一个白球，我们就会判断该小球为黑色的概率为 $\frac{3}{4}$，倾向于猜测小球为黑色，但如果在答案公示前，有知情人员告诉我们（没告诉其他人）抓到的是白球，我们掌握的信息量就会很大（相当于直接确定）并远远大于其他观众；而如果知情人员告诉我们抓到的是黑球，与我们根据已知信息做出的倾向性预测相同，此时我们虽然掌握的信息量也增加了（从比较确定变成确定），但是并没有前一种情况增加的多，所获得的信息也没有比其他人多很多。需要注意的是，信息量只是描述未知状态下的不确定性，并不和未知的真实结果相关。上述案例中，如果抓到的小球是白色，那大部分观众就会猜错，甚至比完全不知道小球颜色数量时猜错的概率更大，但是掌握的信息仍然是多于一无所知时候的。

&emsp;&emsp;上述案例可以看出，概率可以很清晰的描述信息量。从概率角度度量信息的不确定性，考虑一个随机变量 $x$ ，我们对信息内容的度量依赖概率分布 $p(x)$，然后寻找⼀个函数 $h(x)$，它是概率 $p(x)$ 的单调递增函数，根据我们已知的概率大小表达信息量。$h(x)$ 可以这样定义：对于两个不相关的事件 $a$ 和 $b$，我们观察到两个事件同时发⽣时获得的信息应该等于事件各⾃发⽣时获得的信息之和，即 $h(a,b)=h(a)+h(b)$。两个事件是统计独⽴的，因此 $p(a,b)=p(a)p(b)$。根据这两个关系，很容易构造出 $h(x)$ 与 $p(x)$ 的对数关系

$$h(x)=-\log_2p(x)\tag1$$
其中，负号确保了信息的非负特性。低概率事件 $x$ 对应于⾼的信息量，正如前面案例在已知小球颜色数量的情况下抓到白球的小概率事件包含着更丰富的信息。对数的底的选择是任意的。如果遵循信息论的普遍传统，就会使⽤ 2 作为对数的底，此时，$h(x)$ 的单位是⽐特 (bit, binary digit)。大家对计算机中的 bit 相信一定不陌生，它和内存相关，而计算机内存正好表示所存储的信息量，关于这个信息量的描述，有一个更通俗的例子。

#### 示例二：猜数字游戏 - 信息是如何被量化的

&emsp;&emsp;两个人 A 和 B 一起玩一个猜数字游戏，A 随机想一个 32 以内的整数（不包含 0），然后 B 来猜。游戏规定 A 可以回答 B 的问题，比如 B 猜的数字是否比 A 所想的数字要大，我们想想 B 最少提问多少次后，就可以准确猜出 A 所想的数字。B 只需要先问 ”是否大于 16“，如果 A 回答不是，就再问 "是否大于 8"，如果 A 回答是，再问 "是否大于 12"，依次类推，只需要提问 5 次就一定可以准确知道是哪个数字。很容易看出这与二进制是一致的，提问顺序就是依次从高位到低位确定二进制每一位。也就是说，在计算机里储存一个 32 以内（不包含 32）的整数，只需要 5 比特的内存；而如果是描述一个在 $[0,31]$ 上均匀分布的整数这样一个事件的话，它所包含的信息量也是 5 比特。当然，计算机存储的就是信息，而这样一个有 32 种可能情况的事件包含 5 比特信息，恰好也是对数关系，我们再次将概率分布和信息量用同一种表示方式联系起来。

&emsp;&emsp;但是如果 A 在想出这个整数时并不是在 $[1,32]$ 均匀采样，假设 B 已知 A 有 $\frac{2}{3}$ 的可能想出 $[1,16]$ 上的数字，此时他第一次就询问 "是否大于 8"，如果 A 的数字确实是在 $[1,16]$ 上，那么 B 一共只需提问 4 次就可以知道，但是如果该数字在 $[17,32]$ 上，B 需要提问 6 次才能得知。乍看之下，好像信息量也没减少，但如果结合已知的概率分布计算信息量期望，这个事件所包含的信息量应该是 $\frac{2}{3}\times 4+\frac{1}{3}\times 6=\frac{14}{3} \mathrm {bit}<5\mathrm {bit}$，可见我们所掌握的先验知识相比均匀采样的情况多了 $\frac{1}{3}\mathrm{bit}$ 的信息量。当然，这个例子最终还是为了说明，描述信息量需要考虑概率分布的期望。

&emsp;&emsp;现在假设⼀个发送者想传输⼀个随机的离散变量 x 给接收者，离散变量概率分布为 p(x) 。此时传输的平均信息量通通过公式 (1) 关于 p(x) 的期望得到，期望值为

$$H[x]=-\sum_xp(x)\log_2p(x)\tag2$$
这个量被叫做随机变量 $x$ 的熵 (entropy)，需要注意，$\lim_{p\rightarrow \infty}p\log_2 p=0$。

#### 示例三：汽车山羊游戏 - 信息是如何消除不确定性的

https://www.zhihu.com/question/26709273

#### 示例四：战场情报 - 信息的价值作用有多大

&emsp;&emsp;我们将第三次科技革命后的时代称为信息时代，所有人身处在交错复杂的互联网中，对信息掌握的差异形成信息差，影响了很多人的职业选择乃至生涯规划，多种不同形式的竞争本质上就是一种信息战，丰富的信息往往会占据主动的话语权。正如孙子兵法所云：“知己知彼，百战不殆”，在战争中，信息的作用尤为重要。
信息的英文是 information，同样也有 “情报” 的意思，CIA，军情六处，克格勃的故事至今仍为人津津乐道，翻拍的各类影视剧层出不穷，那么战争中谍战或情报战意义究竟有多重大呢？1941 年的苏德战场，纳粹德军势如破竹，一度将战线推进到莫斯科城下，苏联俨然兵力不继，虽然在西伯利亚仍有 60 万苏联红军驻军，但慈父同志斯大林，虽然，但为了防止日本入侵不敢轻易调离，因为斯大林不知道日军将北上进攻苏联，还是南下向美国开战。这时传奇间谍佐尔格向斯大林发送一则我们看来只有 1 比特的情报，“日军将南下”，于是斯大林大胆地从东线抽调了 26 个精良师增援西线，莫斯科保卫战一举扭转战局，改变了二战进程。

https://new.qq.com/rain/a/20220525A02TTA00

## 1.3 自然语言的信息熵 The Entropy of Natural Language

&emsp;&emsp;在机器学习中，我们常把信息论用于描述数据和模型的不确定性，随机噪声通常被假设为高斯分布就是因为它具有最大的不确定性，为了度量这种不确定性，就需要引入信息熵的概念，机器学习的目标就是要降低模型对数据特征描述和预测的不确定性，误差函数就是熵的一种描述，优化误差函数其实就是一个熵减的过程。


&emsp;&emsp;有了熵这个概念，我们就尝试把它用在更贴近我们实际生活的事情上，比如说，我们想知道一篇 500 字的作文信息熵是多少，首先我们知道，这篇作文是由一些常用汉字组成的，如果常用汉字有 7000 个，且每一个汉字出现概率相同，那么根据公式 (2) 信息熵为 $\frac{1}{7000}\times\log_2 \frac{1}{7000}\times500\approx6500\mathrm {bit}$。但实际上每一个汉字的使用频率是不相等的，这就相当于引入先验知识，这种情况这篇文章包含的信息小于 $6500\mathrm {bit}$。到这我们又可以发现，引入先验知识后信息熵总会下降，而公式 (2) 取最大值的时候恰好也是每一个结果都等概率出现的时候。因为不借助任何先验知识，我们总会等概率的猜测每一种结果出现的可能性，这个时候就是不确定性最高的。有使用过机器学习或深度学习做分类的同学应该会发现，在我们对模型参数做了初始化后不进行学习直接分类，对于一个十分类的问题，起始准确率总在 $10\%$ 左右，这个时候也没进行学习，意味着我们没有获得任何信息。但是这并不意味着均匀分布的不确定性就最大，后面在高斯分布一节中，我们会证明在均值和方差一定时，高斯分布拥有最大熵。

&emsp;&emsp;如果我们把上述例子的的作文输入到计算机，同样也需要 $6500\mathrm {bit}$ 内存，但是我们已知这篇作文实际信息熵是小于这个值的，这里就造成了信息的冗余 (redundancy)，这在信息编码里是不可或缺的。值得一提的是，所有语言里，汉语的冗余度是相对较小的，同一段文字翻译成中英两种版本，中文总是要简洁一些。

https://www.zhihu.com/question/37998688

https://www.zhihu.com/question/27403427

https://zhuanlan.zhihu.com/p/89958871

https://zhuanlan.zhihu.com/p/81489867

https://kknews.cc/zh-my/education/q9oox9y.html